{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb1f7518",
   "metadata": {},
   "source": [
    "## Langchain:\n",
    "\n",
    "LangChain is an open source framework with a pre-built agent architecture and integrations for any model or tool — so you can build agents that adapt as fast as the ecosystem evolves\n",
    "\n",
    "Use LangGraph, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\n",
    "\n",
    "LangChain agents are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n",
    "\n",
    "Ref: https://docs.langchain.com/oss/python/langchain/overview\n",
    "\n",
    "LangChain is driven by a few core beliefs:\n",
    "1. Large Language Models (LLMs) are great, powerful new technology.\n",
    "2. LLMs are even better when you combine them with external sources of data.\n",
    "3. LLMs will transform what the applications of the future look like. Specifically, the applications of the future will look more and more agentic.\n",
    "4. It is still very early on in that transformation.\n",
    "5. While it’s easy to build a prototype of those agentic applications, it’s still really hard to build agents that are reliable enough to put into production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea804bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -qU langchain \"langchain[anthropic]\"\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"claude-sonnet-4-5-20250929\",\n",
    "    tools=[get_weather],\n",
    "    system_prompt=\"You are a helpful assistant\",\n",
    ")\n",
    "\n",
    "# Run the agent\n",
    "agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7442f4",
   "metadata": {},
   "source": [
    "### Create and Run the agents:\n",
    "\n",
    "Build a practical weather forecasting agent that demonstrates key production concepts:\n",
    "1. Detailed system prompts for better agent behavior\n",
    "2. Create tools that integrate with external data\n",
    "3. Model configuration for consistent responses\n",
    "4. Structured output for predictable results\n",
    "5. Conversational memory for chat-like interactions\n",
    "6. Create and run the agent create a fully functional agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0de5dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert weather forecaster, who speaks in puns.\n",
    "\n",
    "You have access to two tools:\n",
    "\n",
    "- get_weather_for_location: use this to get the weather for a specific location\n",
    "- get_user_location: use this to get the user's location\n",
    "\n",
    "If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location.\"\"\"\n",
    "\n",
    "@tool\n",
    "def get_weather_for_location(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "@dataclass\n",
    "class Context:\n",
    "    \"\"\"Custom runtime context schema.\"\"\"\n",
    "    user_id: str\n",
    "\n",
    "@tool\n",
    "def get_user_location(runtime: ToolRuntime[Context]) -> str:\n",
    "    \"\"\"Retrieve user information based on user ID.\"\"\"\n",
    "    user_id = runtime.context.user_id\n",
    "    return \"Florida\" if user_id == \"1\" else \"SF\"\n",
    "\n",
    "model = init_chat_model(\n",
    "    \"claude-sonnet-4-5-20250929\",\n",
    "    temperature=0.5,\n",
    "    timeout=10,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "# We use a dataclass here, but Pydantic models are also supported.\n",
    "@dataclass\n",
    "class ResponseFormat:\n",
    "    \"\"\"Response schema for the agent.\"\"\"\n",
    "    # A punny response (always required)\n",
    "    punny_response: str\n",
    "    # Any interesting information about the weather if available\n",
    "    weather_conditions: str | None = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f45ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.structured_output import ToolStrategy\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    tools=[get_user_location, get_weather_for_location],\n",
    "    context_schema=Context,\n",
    "    response_format=ToolStrategy(ResponseFormat),\n",
    "    checkpointer=checkpointer\n",
    ")\n",
    "\n",
    "# `thread_id` is a unique identifier for a given conversation.\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather outside?\"}]},\n",
    "    config=config,\n",
    "    context=Context(user_id=\"1\")\n",
    ")\n",
    "\n",
    "print(response['structured_response'])\n",
    "# ResponseFormat(\n",
    "#     punny_response=\"Florida is still having a 'sun-derful' day! The sunshine is playing 'ray-dio' hits all day long! I'd say it's the perfect weather for some 'solar-bration'! If you were hoping for rain, I'm afraid that idea is all 'washed up' - the forecast remains 'clear-ly' brilliant!\",\n",
    "#     weather_conditions=\"It's always sunny in Florida!\"\n",
    "# )\n",
    "\n",
    "\n",
    "# Note that we can continue the conversation using the same `thread_id`.\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"thank you!\"}]},\n",
    "    config=config,\n",
    "    context=Context(user_id=\"1\")\n",
    ")\n",
    "\n",
    "print(response['structured_response'])\n",
    "# ResponseFormat(\n",
    "#     punny_response=\"You're 'thund-erfully' welcome! It's always a 'breeze' to help you stay 'current' with the weather. I'm just 'cloud'-ing around waiting to 'shower' you with more forecasts whenever you need them. Have a 'sun-sational' day in the Florida sunshine!\",\n",
    "#     weather_conditions=None\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7977805c",
   "metadata": {},
   "source": [
    "### Middlewares:\n",
    "\n",
    "Middleware provides powerful extensibility for customizing agent behavior at different stages of execution. You can use middleware to:\n",
    "1. Process state before the model is called (e.g., message trimming, context injection)\n",
    "2. Modify or validate the model’s response (e.g., guardrails, content filtering)\n",
    "3. Handle tool execution errors with custom logic\n",
    "4. Implement dynamic model selection based on state or context\n",
    "5. Add custom logging, monitoring, or analytics\n",
    "6. Middleware integrates seamlessly into the agent’s execution, allowing you to intercept and modify data flow at key points without changing the core agent logic.\n",
    "\n",
    "#### @wrap_model_call: \n",
    "In Agents, Dynamic models are selected at runtime based on the current state and context. This enables sophisticated routing logic and cost optimization.\n",
    "To use a dynamic model, create middleware using the @wrap_model_call decorator that modifies the model in the request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be43733f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n",
    "\n",
    "\n",
    "basic_model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "advanced_model = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "@wrap_model_call\n",
    "def dynamic_model_selection(request: ModelRequest, handler) -> ModelResponse:\n",
    "    \"\"\"Choose model based on conversation complexity.\"\"\"\n",
    "    message_count = len(request.state[\"messages\"])\n",
    "\n",
    "    if message_count > 10:\n",
    "        # Use an advanced model for longer conversations\n",
    "        model = advanced_model\n",
    "    else:\n",
    "        model = basic_model\n",
    "\n",
    "    return handler(request.override(model=model))\n",
    "\n",
    "agent = create_agent(\n",
    "    model=basic_model,  # Default model\n",
    "    tools=tools,\n",
    "    middleware=[dynamic_model_selection]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d905a545",
   "metadata": {},
   "source": [
    "#### @wrap_tool_call:\n",
    "\n",
    "To customize how tool errors are handled, use the @wrap_tool_call decorator to create middleware:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db0057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import wrap_tool_call\n",
    "from langchain.messages import ToolMessage\n",
    "\n",
    "\n",
    "@wrap_tool_call\n",
    "def handle_tool_errors(request, handler):\n",
    "    \"\"\"Handle tool execution errors with custom messages.\"\"\"\n",
    "    try:\n",
    "        return handler(request)\n",
    "    except Exception as e:\n",
    "        # Return a custom error message to the model\n",
    "        return ToolMessage(\n",
    "            content=f\"Tool error: Please check your input and try again. ({str(e)})\",\n",
    "            tool_call_id=request.tool_call[\"id\"]\n",
    "        )\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[search, get_weather],\n",
    "    middleware=[handle_tool_errors]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e87e10",
   "metadata": {},
   "source": [
    "### Dyanmic System Prompt:\n",
    "\n",
    "#### @dynamic_prompt:\n",
    "The @dynamic_prompt decorator creates middleware that generates system prompts based on the model request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158ff131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "\n",
    "\n",
    "class Context(TypedDict):\n",
    "    user_role: str\n",
    "\n",
    "@dynamic_prompt\n",
    "def user_role_prompt(request: ModelRequest) -> str:\n",
    "    \"\"\"Generate system prompt based on user role.\"\"\"\n",
    "    user_role = request.runtime.context.get(\"user_role\", \"user\")\n",
    "    base_prompt = \"You are a helpful assistant.\"\n",
    "\n",
    "    if user_role == \"expert\":\n",
    "        return f\"{base_prompt} Provide detailed technical responses.\"\n",
    "    elif user_role == \"beginner\":\n",
    "        return f\"{base_prompt} Explain concepts simply and avoid jargon.\"\n",
    "\n",
    "    return base_prompt\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[web_search],\n",
    "    middleware=[user_role_prompt],\n",
    "    context_schema=Context\n",
    ")\n",
    "\n",
    "# The system prompt will be set dynamically based on context\n",
    "result = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Explain machine learning\"}]},\n",
    "    context={\"user_role\": \"expert\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fcd37e",
   "metadata": {},
   "source": [
    "Custom state schemas must extend AgentState as a TypedDict.\n",
    "\n",
    "There are two ways to define custom state:\n",
    "1. Via middleware (preferred)\n",
    "2. Via state_schema on create_agent\n",
    "\n",
    "\n",
    "#### Defining state via middleware\n",
    "\n",
    "Use middleware to define custom state when your custom state needs to be accessed by specific middleware hooks and tools attached to said middleware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e16b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentState\n",
    "from langchain.agents.middleware import AgentMiddleware\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "class CustomState(AgentState):\n",
    "    user_preferences: dict\n",
    "\n",
    "class CustomMiddleware(AgentMiddleware):\n",
    "    state_schema = CustomState\n",
    "    tools = [tool1, tool2]\n",
    "\n",
    "    def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n",
    "        ...\n",
    "\n",
    "agent = create_agent(\n",
    "    model,\n",
    "    tools=tools,\n",
    "    middleware=[CustomMiddleware()]\n",
    ")\n",
    "\n",
    "# The agent can now track additional state beyond messages\n",
    "result = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"I prefer technical explanations\"}],\n",
    "    \"user_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"},\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce388a5b",
   "metadata": {},
   "source": [
    "\n",
    "#### Structured Output\n",
    "\n",
    "In some situations, you may want the agent to return an output in a specific format. LangChain provides strategies for structured output via the 'response_format' parameter.\n",
    "\n",
    "##### ToolStrategy\n",
    "ToolStrategy uses artificial tool calling to generate structured output. This works with any model that supports tool calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41978800",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.structured_output import ToolStrategy\n",
    "\n",
    "\n",
    "class ContactInfo(BaseModel):\n",
    "    name: str\n",
    "    email: str\n",
    "    phone: str\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[search_tool],\n",
    "    response_format=ToolStrategy(ContactInfo)\n",
    ")\n",
    "\n",
    "result = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}]\n",
    "})\n",
    "\n",
    "result[\"structured_response\"]\n",
    "# ContactInfo(name='John Doe', email='john@example.com', phone='(555) 123-4567')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674e7eed",
   "metadata": {},
   "source": [
    "### ProviderStrategy:\n",
    "\n",
    "ProviderStrategy uses the model provider’s native structured output generation. This is more reliable but only works with providers that support native structured output (e.g., OpenAI):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f0662a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.structured_output import ProviderStrategy\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    response_format=ProviderStrategy(ContactInfo)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286fd393",
   "metadata": {},
   "source": [
    "### Streaming:\n",
    "\n",
    "We’ve seen how the agent can be called with invoke to get a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584f8752",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in agent.stream({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Search for AI news and summarize the findings\"}]\n",
    "}, stream_mode=\"values\"):\n",
    "    # Each chunk contains the full state at that point\n",
    "    latest_message = chunk[\"messages\"][-1]\n",
    "    if latest_message.content:\n",
    "        print(f\"Agent: {latest_message.content}\")\n",
    "    elif latest_message.tool_calls:\n",
    "        print(f\"Calling tools: {[tc['name'] for tc in latest_message.tool_calls]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4c4b45",
   "metadata": {},
   "source": [
    "### Batch\n",
    "\n",
    "Batching a collection of independent requests to a model can significantly improve performance and reduce costs, as the processing can be done in parallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac83037",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = model.batch([\n",
    "    \"Why do parrots have colorful feathers?\",\n",
    "    \"How do airplanes fly?\",\n",
    "    \"What is quantum computing?\"\n",
    "])\n",
    "for response in responses:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a93a973",
   "metadata": {},
   "source": [
    "By default, batch() will only return the final output for the entire batch. If you want to receive the output for each individual input as it finishes generating, you can stream results with batch_as_completed():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9842dc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for response in model.batch_as_completed([\n",
    "    \"Why do parrots have colorful feathers?\",\n",
    "    \"How do airplanes fly?\",\n",
    "    \"What is quantum computing?\"\n",
    "]):\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88af237",
   "metadata": {},
   "source": [
    "When processing a large number of inputs using batch() or batch_as_completed(), you may want to control the maximum number of parallel calls. This can be done by setting the max_concurrency attribute in the RunnableConfig dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1412892",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.batch(\n",
    "    list_of_inputs,\n",
    "    config={\n",
    "        'max_concurrency': 5,  # Limit to 5 parallel calls\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5aae1f",
   "metadata": {},
   "source": [
    "### Tool calling\n",
    "\n",
    "Models can request to call tools that perform tasks such as fetching data from a database, searching the web, or running code. Tools are pairings of:\n",
    "1. A schema, including the name of the tool, a description, and/or argument definitions (often a JSON schema)\n",
    "2. A function or coroutine to execute.\n",
    "\n",
    "To make tools that you have defined available for use by a model, you must bind them using bind_tools. In subsequent invocations, the model can choose to call any of the bound tools as needed.\n",
    "\n",
    "By default, the model has the freedom to choose which bound tool to use based on the user’s input. However, you might want to force choosing a tool, ensuring the model uses either a particular tool or any tool from a given list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac2954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force use of any tool\n",
    "model_with_tools = model.bind_tools([tool_1], tool_choice=\"any\")\n",
    "\n",
    "# Force use of specific tools\n",
    "model_with_tools = model.bind_tools([tool_1], tool_choice=\"tool_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e5cb33",
   "metadata": {},
   "source": [
    "Many models support calling multiple tools in parallel when appropriate. This allows the model to gather information from different sources simultaneously.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a98c517",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_tools = model.bind_tools([get_weather])\n",
    "\n",
    "response = model_with_tools.invoke(\n",
    "    \"What's the weather in Boston and Tokyo?\"\n",
    ")\n",
    "\n",
    "\n",
    "# The model may generate multiple tool calls\n",
    "print(response.tool_calls)\n",
    "# [\n",
    "#   {'name': 'get_weather', 'args': {'location': 'Boston'}, 'id': 'call_1'},\n",
    "#   {'name': 'get_weather', 'args': {'location': 'Tokyo'}, 'id': 'call_2'},\n",
    "# ]\n",
    "\n",
    "\n",
    "# Execute all tools (can be done in parallel with async)\n",
    "results = []\n",
    "for tool_call in response.tool_calls:\n",
    "    if tool_call['name'] == 'get_weather':\n",
    "        result = get_weather.invoke(tool_call)\n",
    "    ...\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2757fa",
   "metadata": {},
   "source": [
    "Most models supporting tool calling enable parallel tool calls by default. Some (including OpenAI and Anthropic) allow you to disable this feature. To do this, set parallel_tool_calls=False:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d318d6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.bind_tools([get_weather], parallel_tool_calls=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1c52f4",
   "metadata": {},
   "source": [
    "### Structured Output:\n",
    "\n",
    "Models can be requested to provide their response in a format matching a given schema. This is useful for ensuring the output can be easily parsed and used in subsequent processing. LangChain supports multiple schema types and methods for enforcing structured output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b6a6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Movie(BaseModel):\n",
    "    \"\"\"A movie with details.\"\"\"\n",
    "    title: str = Field(..., description=\"The title of the movie\")\n",
    "    year: int = Field(..., description=\"The year the movie was released\")\n",
    "    director: str = Field(..., description=\"The director of the movie\")\n",
    "    rating: float = Field(..., description=\"The movie's rating out of 10\")\n",
    "\n",
    "model_with_structure = model.with_structured_output(Movie)\n",
    "response = model_with_structure.invoke(\"Provide details about the movie Inception\")\n",
    "print(response)  # Movie(title=\"Inception\", year=2010, director=\"Christopher Nolan\", rating=8.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89f0048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_schema = {\n",
    "    \"title\": \"Movie\",\n",
    "    \"description\": \"A movie with details\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"title\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The title of the movie\"\n",
    "        },\n",
    "        \"year\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"The year the movie was released\"\n",
    "        },\n",
    "        \"director\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The director of the movie\"\n",
    "        },\n",
    "        \"rating\": {\n",
    "            \"type\": \"number\",\n",
    "            \"description\": \"The movie's rating out of 10\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"title\", \"year\", \"director\", \"rating\"]\n",
    "}\n",
    "\n",
    "model_with_structure = model.with_structured_output(\n",
    "    json_schema,\n",
    "    method=\"json_schema\",\n",
    ")\n",
    "response = model_with_structure.invoke(\"Provide details about the movie Inception\")\n",
    "print(response)  # {'title': 'Inception', 'year': 2010, ...}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff06382b",
   "metadata": {},
   "source": [
    "#### Message output alongside parsed structure\n",
    "\n",
    "It can be useful to return the raw AIMessage object alongside the parsed representation to access response metadata such as token counts. To do this, set include_raw=True when calling with_structured_output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429b2db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Movie(BaseModel):\n",
    "    \"\"\"A movie with details.\"\"\"\n",
    "    title: str = Field(..., description=\"The title of the movie\")\n",
    "    year: int = Field(..., description=\"The year the movie was released\")\n",
    "    director: str = Field(..., description=\"The director of the movie\")\n",
    "    rating: float = Field(..., description=\"The movie's rating out of 10\")\n",
    "\n",
    "model_with_structure = model.with_structured_output(Movie, include_raw=True)  \n",
    "response = model_with_structure.invoke(\"Provide details about the movie Inception\")\n",
    "response\n",
    "# {\n",
    "#     \"raw\": AIMessage(...),\n",
    "#     \"parsed\": Movie(title=..., year=..., ...),\n",
    "#     \"parsing_error\": None,\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ea45e3",
   "metadata": {},
   "source": [
    "### Token Usage\n",
    "\n",
    "A number of model providers return token usage information as part of the invocation response. When available, this information will be included on the AIMessage objects produced by the corresponding model.\n",
    "\n",
    "You can track aggregate token counts across models in an application using either a callback or context manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2123b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback Handler\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.callbacks import UsageMetadataCallbackHandler\n",
    "\n",
    "model_1 = init_chat_model(model=\"gpt-4o-mini\")\n",
    "model_2 = init_chat_model(model=\"claude-haiku-4-5-20251001\")\n",
    "\n",
    "callback = UsageMetadataCallbackHandler()\n",
    "result_1 = model_1.invoke(\"Hello\", config={\"callbacks\": [callback]})\n",
    "result_2 = model_2.invoke(\"Hello\", config={\"callbacks\": [callback]})\n",
    "callback.usage_metadata\n",
    "\n",
    "# Context Manager\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.callbacks import get_usage_metadata_callback\n",
    "\n",
    "model_1 = init_chat_model(model=\"gpt-4o-mini\")\n",
    "model_2 = init_chat_model(model=\"claude-haiku-4-5-20251001\")\n",
    "\n",
    "with get_usage_metadata_callback() as cb:\n",
    "    model_1.invoke(\"Hello\")\n",
    "    model_2.invoke(\"Hello\")\n",
    "    print(cb.usage_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eabcdf0",
   "metadata": {},
   "source": [
    "#### Invocation config\n",
    "\n",
    "When invoking a model, you can pass additional configuration through the config parameter using a RunnableConfig dictionary. This provides run-time control over execution behavior, callbacks, and metadata tracking.\n",
    "\n",
    "These configuration values are particularly useful when:\n",
    "1. Debugging with LangSmith tracing\n",
    "2. Implementing custom logging or monitoring\n",
    "3. Controlling resource usage in production\n",
    "4. Tracking invocations across complex pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be917cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.invoke(\n",
    "    \"Tell me a joke\",\n",
    "    config={\n",
    "        \"run_name\": \"joke_generation\",      # Custom name for this run\n",
    "        \"tags\": [\"humor\", \"demo\"],          # Tags for categorization\n",
    "        \"metadata\": {\"user_id\": \"123\"},     # Custom metadata\n",
    "        \"callbacks\": [my_callback_handler], # Callback handlers\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4872796c",
   "metadata": {},
   "source": [
    "### Messages\n",
    "\n",
    "Messages are the fundamental unit of context for models in LangChain. They represent the input and output of models, carrying both the content and metadata needed to represent the state of a conversation when interacting with an LLM.\n",
    "\n",
    "Messages are objects that contain:\n",
    "1. Role - Identifies the message type (e.g. system, user)\n",
    "2. Content - Represents the actual content of the message (like text, images, audio, documents, etc.)\n",
    "3. Metadata - Optional fields such as response information, message IDs, and token usage\n",
    "\n",
    "LangChain provides a standard message type that works across all model providers, ensuring consistent behavior regardless of the model being called.\n",
    "\n",
    "​\n",
    "Message types\n",
    "1. System message - Tells the model how to behave and provide context for interactions\n",
    "2. Human message - Represents user input and interactions with the model\n",
    "3. AI message - Responses generated by the model, including text content, tool calls, and metadata\n",
    "4. Tool message - Represents the outputs of tool calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a87cef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"You are a poetry expert\"),\n",
    "    HumanMessage(\"Write a haiku about spring\"),\n",
    "    AIMessage(\"Cherry blossoms bloom...\")\n",
    "]\n",
    "response = model.invoke(messages)\n",
    "\n",
    "##################################\n",
    "\n",
    "# Execute tool and create result message\n",
    "weather_result = \"Sunny, 72°F\"\n",
    "tool_message = ToolMessage(\n",
    "    content=weather_result,\n",
    "    tool_call_id=\"call_123\"  # Must match the call ID\n",
    ")\n",
    "\n",
    "# Continue conversation\n",
    "messages = [\n",
    "    HumanMessage(\"What's the weather in San Francisco?\"),\n",
    "    ai_message,  # Model's tool call\n",
    "    tool_message,  # Tool execution result\n",
    "]\n",
    "response = model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72f0192",
   "metadata": {},
   "source": [
    "### Multimodal\n",
    "\n",
    "Multimodality refers to the ability to work with data that comes in different forms, such as text, audio, images, and video. LangChain includes standard types for these data that can be used across providers. Chat models can accept multimodal data as input and generate it as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08745aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From URL\n",
    "message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"Describe the content of this image.\"},\n",
    "        {\"type\": \"image\", \"url\": \"https://example.com/path/to/image.jpg\"},\n",
    "    ]\n",
    "}\n",
    "\n",
    "# From base64 data\n",
    "message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"Describe the content of this image.\"},\n",
    "        {\n",
    "            \"type\": \"image\",\n",
    "            \"base64\": \"AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...\",\n",
    "            \"mime_type\": \"image/jpeg\",\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "# From base64 data\n",
    "message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"Describe the content of this document.\"},\n",
    "        {\n",
    "            \"type\": \"file\",\n",
    "            \"base64\": \"AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...\",\n",
    "            \"mime_type\": \"application/pdf\",\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "# From base64 data\n",
    "message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"Describe the content of this audio.\"},\n",
    "        {\n",
    "            \"type\": \"audio\",\n",
    "            \"base64\": \"AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...\",\n",
    "            \"mime_type\": \"audio/wav\",\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "# From base64 data\n",
    "message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"Describe the content of this video.\"},\n",
    "        {\n",
    "            \"type\": \"video\",\n",
    "            \"base64\": \"AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...\",\n",
    "            \"mime_type\": \"video/mp4\",\n",
    "        },\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66362a2b",
   "metadata": {},
   "source": [
    "### Tools\n",
    "\n",
    "Tools extend what agents can do—letting them fetch real-time data, execute code, query external databases, and take actions in the world.\n",
    "\n",
    "The simplest way to create a tool is with the @tool decorator. By default, the function’s docstring becomes the tool’s description that helps the model understand when to use it.\n",
    "\n",
    "The following parameter names are reserved and cannot be used as tool arguments. Using these names will cause runtime errors.\n",
    "\n",
    "1. config: Reserved for passing RunnableConfig to tools internally\n",
    "2. runtime:\tReserved for ToolRuntime parameter (accessing state, context, store). Runtime context provides a way to inject dependencies (like database connections, user IDs, or configuration) into your tools at runtime, making them more testable and reusable.\n",
    "\n",
    "Tools can access runtime information through the ToolRuntime parameter, which provides:\n",
    "\n",
    "1. State - Mutable data that flows through execution (e.g., messages, counters, custom fields)\n",
    "2. Context - Immutable configuration like user IDs, session details, or application-specific configuration\n",
    "3. Store - Persistent long-term memory across conversations\n",
    "4. Stream Writer - Stream custom updates as tools execute\n",
    "5. Config - RunnableConfig for the execution\n",
    "6. Tool Call ID - ID of the current tool call\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fa83bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool, ToolRuntime\n",
    "\n",
    "# Access the current conversation state\n",
    "@tool\n",
    "def summarize_conversation(\n",
    "    runtime: ToolRuntime\n",
    ") -> str:\n",
    "    \"\"\"Summarize the conversation so far.\"\"\"\n",
    "    messages = runtime.state[\"messages\"]\n",
    "\n",
    "    human_msgs = sum(1 for m in messages if m.__class__.__name__ == \"HumanMessage\")\n",
    "    ai_msgs = sum(1 for m in messages if m.__class__.__name__ == \"AIMessage\")\n",
    "    tool_msgs = sum(1 for m in messages if m.__class__.__name__ == \"ToolMessage\")\n",
    "\n",
    "    return f\"Conversation has {human_msgs} user messages, {ai_msgs} AI responses, and {tool_msgs} tool results\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f7e1b8",
   "metadata": {},
   "source": [
    "### Updating state:\n",
    "\n",
    "Use Command to update the agent’s state or control the graph’s execution flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589d5279",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import Command\n",
    "from langchain.messages import RemoveMessage\n",
    "from langgraph.graph.message import REMOVE_ALL_MESSAGES\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "\n",
    "# Update the conversation history by removing all messages\n",
    "@tool\n",
    "def clear_conversation() -> Command:\n",
    "    \"\"\"Clear the conversation history.\"\"\"\n",
    "\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [RemoveMessage(id=REMOVE_ALL_MESSAGES)],\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Update the user_name in the agent state\n",
    "@tool\n",
    "def update_user_name(\n",
    "    new_name: str,\n",
    "    runtime: ToolRuntime\n",
    ") -> Command:\n",
    "    \"\"\"Update the user's name.\"\"\"\n",
    "    return Command(update={\"user_name\": new_name})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29418760",
   "metadata": {},
   "source": [
    "### Context\n",
    "\n",
    "Access immutable configuration and contextual data like user IDs, session details, or application-specific configuration through runtime.context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2c9626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_agent\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "\n",
    "\n",
    "USER_DATABASE = {\n",
    "    \"user123\": {\n",
    "        \"name\": \"Alice Johnson\",\n",
    "        \"account_type\": \"Premium\",\n",
    "        \"balance\": 5000,\n",
    "        \"email\": \"alice@example.com\"\n",
    "    },\n",
    "    \"user456\": {\n",
    "        \"name\": \"Bob Smith\",\n",
    "        \"account_type\": \"Standard\",\n",
    "        \"balance\": 1200,\n",
    "        \"email\": \"bob@example.com\"\n",
    "    }\n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class UserContext:\n",
    "    user_id: str\n",
    "\n",
    "@tool\n",
    "def get_account_info(runtime: ToolRuntime[UserContext]) -> str:\n",
    "    \"\"\"Get the current user's account information.\"\"\"\n",
    "    user_id = runtime.context.user_id\n",
    "\n",
    "    if user_id in USER_DATABASE:\n",
    "        user = USER_DATABASE[user_id]\n",
    "        return f\"Account holder: {user['name']}\\nType: {user['account_type']}\\nBalance: ${user['balance']}\"\n",
    "    return \"User not found\"\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "agent = create_agent(\n",
    "    model,\n",
    "    tools=[get_account_info],\n",
    "    context_schema=UserContext,\n",
    "    system_prompt=\"You are a financial assistant.\"\n",
    ")\n",
    "\n",
    "result = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What's my current balance?\"}]},\n",
    "    context=UserContext(user_id=\"user123\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0532eb",
   "metadata": {},
   "source": [
    "### Memory (Store)\n",
    "\n",
    "Access persistent data across conversations using the store. The store is accessed via runtime.store and allows you to save and retrieve user-specific or application-specific data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629a524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langchain.agents import create_agent\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "\n",
    "\n",
    "# Access memory\n",
    "@tool\n",
    "def get_user_info(user_id: str, runtime: ToolRuntime) -> str:\n",
    "    \"\"\"Look up user info.\"\"\"\n",
    "    store = runtime.store\n",
    "    user_info = store.get((\"users\",), user_id)\n",
    "    return str(user_info.value) if user_info else \"Unknown user\"\n",
    "\n",
    "# Update memory\n",
    "@tool\n",
    "def save_user_info(user_id: str, user_info: dict[str, Any], runtime: ToolRuntime) -> str:\n",
    "    \"\"\"Save user info.\"\"\"\n",
    "    store = runtime.store\n",
    "    store.put((\"users\",), user_id, user_info)\n",
    "    return \"Successfully saved user info.\"\n",
    "\n",
    "store = InMemoryStore()\n",
    "agent = create_agent(\n",
    "    model,\n",
    "    tools=[get_user_info, save_user_info],\n",
    "    store=store\n",
    ")\n",
    "\n",
    "# First session: save user info\n",
    "agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Save the following user: userid: abc123, name: Foo, age: 25, email: foo@langchain.dev\"}]\n",
    "})\n",
    "\n",
    "# Second session: get user info\n",
    "agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Get user info for user with id 'abc123'\"}]\n",
    "})\n",
    "# Here is the user info for user with ID \"abc123\":\n",
    "# - Name: Foo\n",
    "# - Age: 25\n",
    "# - Email: foo@langchain.dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345ce199",
   "metadata": {},
   "source": [
    "### Short-term Memory\n",
    "\n",
    "To add short-term memory (thread-level persistence) to an agent, you need to specify a checkpointer when creating an agent.\n",
    "LangChain’s agent manages short-term memory as a part of your agent’s state.\n",
    "\n",
    "By storing these in the graph’s state, the agent can access the full context for a given conversation while maintaining separation between different threads.\n",
    "\n",
    "State is persisted to a database (or memory) using a checkpointer so the thread can be resumed at any time.\n",
    "\n",
    "Short-term memory updates when the agent is invoked or a step (like a tool call) is completed, and the state is read at the start of each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9dbe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langgraph.checkpoint.memory import InMemorySaver  \n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    \"gpt-5\",\n",
    "    tools=[get_user_info],\n",
    "    checkpointer=InMemorySaver(),  \n",
    ")\n",
    "\n",
    "agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! My name is Bob.\"}]},\n",
    "    {\"configurable\": {\"thread_id\": \"1\"}},  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df66a239",
   "metadata": {},
   "source": [
    "### In production\n",
    "\n",
    "In production, use a checkpointer backed by a database:\n",
    "\n",
    "!pip3 install langgraph-checkpoint-postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18db9bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "from langgraph.checkpoint.postgres import PostgresSaver  \n",
    "\n",
    "DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\n",
    "with PostgresSaver.from_conn_string(DB_URI) as checkpointer:\n",
    "    checkpointer.setup() # auto create tables in PostgresSql\n",
    "    agent = create_agent(\n",
    "        \"gpt-5\",\n",
    "        tools=[get_user_info],\n",
    "        checkpointer=checkpointer,  \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d84107",
   "metadata": {},
   "source": [
    "### Customizing agent memory\n",
    "\n",
    "By default, agents use AgentState to manage short term memory, specifically the conversation history via a messages key.\n",
    "\n",
    "You can extend AgentState to add additional fields. Custom state schemas are passed to create_agent using the state_schema parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c78d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent, AgentState\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "\n",
    "class CustomAgentState(AgentState):  \n",
    "    user_id: str\n",
    "    preferences: dict\n",
    "\n",
    "agent = create_agent(\n",
    "    \"gpt-5\",\n",
    "    tools=[get_user_info],\n",
    "    state_schema=CustomAgentState,  \n",
    "    checkpointer=InMemorySaver(),\n",
    ")\n",
    "\n",
    "# Custom state can be passed in invoke\n",
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "        \"user_id\": \"user_123\",  \n",
    "        \"preferences\": {\"theme\": \"dark\"}  \n",
    "    },\n",
    "    {\"configurable\": {\"thread_id\": \"1\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2626624",
   "metadata": {},
   "source": [
    "### Short-term Memory Common Patterns\n",
    "\n",
    "With short-term memory enabled, long conversations can exceed the LLM’s context window. \n",
    "\n",
    "#### Trim Messages:\n",
    "This allows the agent to keep track of the conversation without exceeding the LLM’s context window.  One way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dca24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import RemoveMessage\n",
    "from langgraph.graph.message import REMOVE_ALL_MESSAGES\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain.agents import create_agent, AgentState\n",
    "from langchain.agents.middleware import before_model\n",
    "from langgraph.runtime import Runtime\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "@before_model\n",
    "def trim_messages(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "    \"\"\"Keep only the last few messages to fit context window.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    if len(messages) <= 3:\n",
    "        return None  # No changes needed\n",
    "\n",
    "    first_msg = messages[0]\n",
    "    recent_messages = messages[-3:] if len(messages) % 2 == 0 else messages[-4:]\n",
    "    new_messages = [first_msg] + recent_messages\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            RemoveMessage(id=REMOVE_ALL_MESSAGES),\n",
    "            *new_messages\n",
    "        ]\n",
    "    }\n",
    "\n",
    "agent = create_agent(\n",
    "    your_model_here,\n",
    "    tools=your_tools_here,\n",
    "    middleware=[trim_messages],\n",
    "    checkpointer=InMemorySaver(),\n",
    ")\n",
    "\n",
    "config: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "agent.invoke({\"messages\": \"hi, my name is bob\"}, config)\n",
    "agent.invoke({\"messages\": \"write a short poem about cats\"}, config)\n",
    "agent.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n",
    "final_response = agent.invoke({\"messages\": \"what's my name?\"}, config)\n",
    "\n",
    "final_response[\"messages\"][-1].pretty_print()\n",
    "\"\"\"\n",
    "================================== Ai Message ==================================\n",
    "\n",
    "Your name is Bob. You told me that earlier.\n",
    "If you'd like me to call you a nickname or use a different name, just say the word.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8430692",
   "metadata": {},
   "source": [
    "### Delete messages\n",
    "\n",
    "You can delete messages from the graph state to manage the message history.\n",
    "This is useful when you want to remove specific messages or clear the entire message history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc696340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import RemoveMessage  \n",
    "\n",
    "def delete_messages(state):\n",
    "    messages = state[\"messages\"]\n",
    "    if len(messages) > 2:\n",
    "        # remove the earliest two messages\n",
    "        return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}\n",
    "\n",
    "# To remove all the messages\n",
    "from langgraph.graph.message import REMOVE_ALL_MESSAGES\n",
    "\n",
    "def delete_messages(state):\n",
    "    return {\"messages\": [RemoveMessage(id=REMOVE_ALL_MESSAGES)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac9f06f",
   "metadata": {},
   "source": [
    "### Summarize messages\n",
    "\n",
    "The problem with trimming or removing messages, as shown above, is that you may lose information from culling of the message queue. Because of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2768ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import SummarizationMiddleware\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[],\n",
    "    middleware=[\n",
    "        SummarizationMiddleware(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            trigger=(\"tokens\", 4000),\n",
    "            keep=(\"messages\", 20)\n",
    "        )\n",
    "    ],\n",
    "    checkpointer=checkpointer,\n",
    ")\n",
    "\n",
    "config: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "agent.invoke({\"messages\": \"hi, my name is bob\"}, config)\n",
    "agent.invoke({\"messages\": \"write a short poem about cats\"}, config)\n",
    "agent.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n",
    "final_response = agent.invoke({\"messages\": \"what's my name?\"}, config)\n",
    "\n",
    "final_response[\"messages\"][-1].pretty_print()\n",
    "\"\"\"\n",
    "================================== Ai Message ==================================\n",
    "\n",
    "Your name is Bob!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3391538",
   "metadata": {},
   "source": [
    "### Write short-term memory from tools\n",
    "\n",
    "To modify the agent’s short-term memory (state) during execution, you can return state updates directly from the tools.\n",
    "This is useful for persisting intermediate results or making information accessible to subsequent tools or prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d036df7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool, ToolRuntime\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain.messages import ToolMessage\n",
    "from langchain.agents import create_agent, AgentState\n",
    "from langgraph.types import Command\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class CustomState(AgentState):  \n",
    "    user_name: str\n",
    "\n",
    "class CustomContext(BaseModel):\n",
    "    user_id: str\n",
    "\n",
    "@tool\n",
    "def update_user_info(\n",
    "    runtime: ToolRuntime[CustomContext, CustomState],\n",
    ") -> Command:\n",
    "    \"\"\"Look up and update user info.\"\"\"\n",
    "    user_id = runtime.context.user_id\n",
    "    name = \"John Smith\" if user_id == \"user_123\" else \"Unknown user\"\n",
    "    return Command(update={  \n",
    "        \"user_name\": name,\n",
    "        # update the message history\n",
    "        \"messages\": [\n",
    "            ToolMessage(\n",
    "                \"Successfully looked up user information\",\n",
    "                tool_call_id=runtime.tool_call_id\n",
    "            )\n",
    "        ]\n",
    "    })\n",
    "\n",
    "@tool\n",
    "def greet(\n",
    "    runtime: ToolRuntime[CustomContext, CustomState]\n",
    ") -> str | Command:\n",
    "    \"\"\"Use this to greet the user once you found their info.\"\"\"\n",
    "    user_name = runtime.state.get(\"user_name\", None)\n",
    "    if user_name is None:\n",
    "       return Command(update={\n",
    "            \"messages\": [\n",
    "                ToolMessage(\n",
    "                    \"Please call the 'update_user_info' tool it will get and update the user's name.\",\n",
    "                    tool_call_id=runtime.tool_call_id\n",
    "                )\n",
    "            ]\n",
    "        })\n",
    "    return f\"Hello {user_name}!\"\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-5-nano\",\n",
    "    tools=[update_user_info, greet],\n",
    "    state_schema=CustomState, \n",
    "    context_schema=CustomContext,\n",
    ")\n",
    "\n",
    "agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"greet the user\"}]},\n",
    "    context=CustomContext(user_id=\"user_123\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4385ff63",
   "metadata": {},
   "source": [
    "#### Prompt\n",
    "\n",
    "Access short term memory (state) in middleware to create dynamic prompts based on conversation history or custom state fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934614bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from typing import TypedDict\n",
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "\n",
    "\n",
    "class CustomContext(TypedDict):\n",
    "    user_name: str\n",
    "\n",
    "\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get the weather in a city.\"\"\"\n",
    "    return f\"The weather in {city} is always sunny!\"\n",
    "\n",
    "\n",
    "@dynamic_prompt\n",
    "def dynamic_system_prompt(request: ModelRequest) -> str:\n",
    "    user_name = request.runtime.context[\"user_name\"]\n",
    "    system_prompt = f\"You are a helpful assistant. Address the user as {user_name}.\"\n",
    "    return system_prompt\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-5-nano\",\n",
    "    tools=[get_weather],\n",
    "    middleware=[dynamic_system_prompt],\n",
    "    context_schema=CustomContext,\n",
    ")\n",
    "\n",
    "result = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n",
    "    context=CustomContext(user_name=\"John Smith\"),\n",
    ")\n",
    "for msg in result[\"messages\"]:\n",
    "    msg.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1344f008",
   "metadata": {},
   "source": [
    "### Before model\n",
    "\n",
    "Access short term memory (state) in @before_model middleware to process messages before model calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0916cac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import RemoveMessage\n",
    "from langgraph.graph.message import REMOVE_ALL_MESSAGES\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain.agents import create_agent, AgentState\n",
    "from langchain.agents.middleware import before_model\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.runtime import Runtime\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "@before_model\n",
    "def trim_messages(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "    \"\"\"Keep only the last few messages to fit context window.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    if len(messages) <= 3:\n",
    "        return None  # No changes needed\n",
    "\n",
    "    first_msg = messages[0]\n",
    "    recent_messages = messages[-3:] if len(messages) % 2 == 0 else messages[-4:]\n",
    "    new_messages = [first_msg] + recent_messages\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            RemoveMessage(id=REMOVE_ALL_MESSAGES),\n",
    "            *new_messages\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    \"gpt-5-nano\",\n",
    "    tools=[],\n",
    "    middleware=[trim_messages],\n",
    "    checkpointer=InMemorySaver()\n",
    ")\n",
    "\n",
    "config: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "agent.invoke({\"messages\": \"hi, my name is bob\"}, config)\n",
    "agent.invoke({\"messages\": \"write a short poem about cats\"}, config)\n",
    "agent.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n",
    "final_response = agent.invoke({\"messages\": \"what's my name?\"}, config)\n",
    "\n",
    "final_response[\"messages\"][-1].pretty_print()\n",
    "\"\"\"\n",
    "================================== Ai Message ==================================\n",
    "\n",
    "Your name is Bob. You told me that earlier.\n",
    "If you'd like me to call you a nickname or use a different name, just say the word.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e9a9ba",
   "metadata": {},
   "source": [
    "### After model\n",
    "\n",
    "Access short term memory (state) in @after_model middleware to process messages after model calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42800c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import RemoveMessage\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain.agents import create_agent, AgentState\n",
    "from langchain.agents.middleware import after_model\n",
    "from langgraph.runtime import Runtime\n",
    "\n",
    "\n",
    "@after_model\n",
    "def validate_response(state: AgentState, runtime: Runtime) -> dict | None:\n",
    "    \"\"\"Remove messages containing sensitive words.\"\"\"\n",
    "    STOP_WORDS = [\"password\", \"secret\"]\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if any(word in last_message.content for word in STOP_WORDS):\n",
    "        return {\"messages\": [RemoveMessage(id=last_message.id)]}\n",
    "    return None\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-5-nano\",\n",
    "    tools=[],\n",
    "    middleware=[validate_response],\n",
    "    checkpointer=InMemorySaver(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa85e572",
   "metadata": {},
   "source": [
    "### Streaming\n",
    "\n",
    "LangChain implements a streaming system to surface real-time updates.\n",
    "Streaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.\n",
    "\n",
    "\n",
    "Supported stream modes:\n",
    "1. updates -> Streams state updates after each agent step. If multiple updates are made in the same step (e.g., multiple nodes are run), those updates are streamed separately.\n",
    "2. messages -> Streams tuples of (token, metadata) from any graph nodes where an LLM is invoked.\n",
    "3. custom -> Streams custom data from inside your graph nodes using the stream writer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5364bab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-5-nano\",\n",
    "    tools=[get_weather],\n",
    ")\n",
    "for chunk in agent.stream(  \n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n",
    "    stream_mode=\"updates\",\n",
    "):\n",
    "    for step, data in chunk.items():\n",
    "        print(f\"step: {step}\")\n",
    "        print(f\"content: {data['messages'][-1].content_blocks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b70668",
   "metadata": {},
   "source": [
    "\n",
    "#### Streaming from sub-agents\n",
    "\n",
    "When there are multiple LLMs at any point in an agent, it’s often necessary to disambiguate the source of messages as they are generated.\n",
    "To do this, you can initialize any model with tags. These tags are then available in metadata when streaming in \"messages\" mode.\n",
    "\n",
    "Below, we update the streaming tool calls example:\n",
    "1. We replace our tool with a call_weather_agent tool that invokes an agent internally\n",
    "2. We add a string tag to this LLM and the outer “supervisor” LLM\n",
    "3. We specify subgraphs=True when creating the stream\n",
    "4. Our stream processing is identical to before, but we add logic to keep track of what LLM is active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45982810",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_message = {\"role\": \"user\", \"content\": \"What is the weather in Boston?\"}\n",
    "current_agent = None\n",
    "for _, stream_mode, data in agent.stream(\n",
    "    {\"messages\": [input_message]},\n",
    "    stream_mode=[\"messages\", \"updates\"],\n",
    "    subgraphs=True,  \n",
    "):\n",
    "    if stream_mode == \"messages\":\n",
    "        token, metadata = data\n",
    "        if tags := metadata.get(\"tags\", []):  \n",
    "            this_agent = tags[0]  \n",
    "            if this_agent != current_agent:  \n",
    "                print(f\"🤖 {this_agent}: \")  \n",
    "                current_agent = this_agent  \n",
    "        if isinstance(token, AIMessage):\n",
    "            _render_message_chunk(token)\n",
    "    if stream_mode == \"updates\":\n",
    "        for source, update in data.items():\n",
    "            if source in (\"model\", \"tools\"):\n",
    "                _render_completed_message(update[\"messages\"][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5054f328",
   "metadata": {},
   "source": [
    "### Disable streaming\n",
    "\n",
    "In some applications you might need to disable streaming of individual tokens for a given model. This is useful when:\n",
    "\n",
    "1. Working with multi-agent systems to control which agents stream their output\n",
    "2. Mixing models that support streaming with those that do not\n",
    "3. Deploying to LangSmith and wanting to prevent certain model outputs from being streamed to the client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e15fc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    streaming=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d93a12",
   "metadata": {},
   "source": [
    "### Middleware:\n",
    "\n",
    "Middleware provides a way to more tightly control what happens inside the agent. Middleware is useful for the following:\n",
    "1. Tracking agent behavior with logging, analytics, and debugging.\n",
    "2. Transforming prompts, tool selection, and output formatting.\n",
    "3. Adding retries, fallbacks, and early termination logic.\n",
    "4. Applying rate limits, guardrails, and PII detection.\n",
    "\n",
    "![alt text](https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w%3D840%26fit%3Dmax%26auto%3Dformat%26n%3DRAP6mjwE5G00xYsA%26q%3D85%26s%3De9b14e264f68345de08ae76f032c52d4)\n",
    "\n",
    "\n",
    "#### Built-in Middleware:\n",
    "\n",
    "https://docs.langchain.com/oss/python/langchain/middleware/built-in\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54366874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarization Middleware\n",
    "\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import SummarizationMiddleware\n",
    "\n",
    "# Multiple conditions: trigger if number of tokens >= 3000 OR messages >= 6\n",
    "agent2 = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[your_weather_tool, your_calculator_tool],\n",
    "    middleware=[\n",
    "        SummarizationMiddleware(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            trigger=[\n",
    "                (\"tokens\", 3000),\n",
    "                (\"messages\", 6),\n",
    "            ],\n",
    "            keep=(\"messages\", 20),\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Human in the loop Middleware\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[your_read_email_tool, your_send_email_tool],\n",
    "    checkpointer=InMemorySaver(),\n",
    "    middleware=[\n",
    "        HumanInTheLoopMiddleware(\n",
    "            interrupt_on={\n",
    "                \"your_send_email_tool\": {\n",
    "                    \"allowed_decisions\": [\"approve\", \"edit\", \"reject\"],\n",
    "                },\n",
    "                \"your_read_email_tool\": False,\n",
    "            }\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# Model call limit Middleware: Limit the number of model calls to prevent infinite loops or excessive costs. \n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    checkpointer=InMemorySaver(),  # Required for thread limiting\n",
    "    tools=[],\n",
    "    middleware=[\n",
    "        ModelCallLimitMiddleware(\n",
    "            thread_limit=10,\n",
    "            run_limit=5,\n",
    "            exit_behavior=\"end\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Tool call limit: Control agent execution by limiting the number of tool calls, either globally across all tools or for specific tools.\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[search_tool, database_tool],\n",
    "    middleware=[\n",
    "        # Global limit\n",
    "        ToolCallLimitMiddleware(thread_limit=20, run_limit=10),\n",
    "        # Tool-specific limit\n",
    "        ToolCallLimitMiddleware(\n",
    "            tool_name=\"search\",\n",
    "            thread_limit=5,\n",
    "            run_limit=3,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Model Fallback Middleware: Automatically fallback to alternative models when the primary model fails.\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[],\n",
    "    middleware=[\n",
    "        ModelFallbackMiddleware(\n",
    "            \"gpt-4o-mini\",\n",
    "            \"claude-3-5-sonnet-20241022\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# To-do list Middleware: Equip agents with task planning and tracking capabilities for complex multi-step tasks. To-do lists are useful for the following:\n",
    "# 1. Complex multi-step tasks requiring coordination across multiple tools.\n",
    "# 2. Long-running operations where progress visibility is important.\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[read_file, write_file, run_tests],\n",
    "    middleware=[TodoListMiddleware()],\n",
    ")\n",
    "\n",
    "# LLM Tool Selection Middleware: Dynamically select the best tool based on the task at hand.\n",
    "# Agents with many tools (10+) where most aren’t relevant per query.Reducing token usage by filtering irrelevant tools.\n",
    "# This middleware uses structured output to ask an LLM which tools are most relevant for the current query. \n",
    "# The structured output schema defines the available tool names and descriptions. \n",
    "# Model providers often add this structured output information to the system prompt behind the scenes.\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[tool1, tool2, tool3, tool4, tool5, ...],\n",
    "    middleware=[\n",
    "        LLMToolSelectorMiddleware(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            max_tools=3,\n",
    "            always_include=[\"search\"],\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Tool Retry Middleware: Automatically retry failed tool calls with configurable exponential backoff\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[search_tool, database_tool],\n",
    "    middleware=[\n",
    "        ToolRetryMiddleware(\n",
    "            max_retries=3,\n",
    "            backoff_factor=2.0,\n",
    "            initial_delay=1.0,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Model retry Middleware: Automatically retry failed model calls with configurable exponential backoff.\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[search_tool, database_tool],\n",
    "    middleware=[\n",
    "        ModelRetryMiddleware(\n",
    "            max_retries=3,\n",
    "            backoff_factor=2.0,\n",
    "            initial_delay=1.0,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# LLM Tool Emulator Middleware: Emulate tool execution using an LLM for testing purposes, replacing actual tool calls with AI-generated responses\n",
    "# Testing agent behavior without executing real tools. Developing agents when external tools are unavailable or expensive.\n",
    " \n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[get_weather, search_database, send_email],\n",
    "    middleware=[\n",
    "        LLMToolEmulator(),  # Emulate all tools\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Emulate specific tools only\n",
    "agent2 = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[get_weather, send_email],\n",
    "    middleware=[LLMToolEmulator(tools=[\"get_weather\"])],\n",
    ")\n",
    "\n",
    "# Use custom model for emulation\n",
    "agent4 = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[get_weather, send_email],\n",
    "    middleware=[LLMToolEmulator(model=\"claude-sonnet-4-5-20250929\")],\n",
    ")\n",
    "\n",
    "# Context Editing Middleware: Manage conversation context by clearing older tool call outputs when token limits are reached, while preserving recent results. \n",
    "# This helps keep context windows manageable in long conversations with many tool calls.\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[],\n",
    "    middleware=[\n",
    "        ContextEditingMiddleware(\n",
    "            edits=[\n",
    "                ClearToolUsesEdit(\n",
    "                    trigger=100000,\n",
    "                    keep=3,\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30389e56",
   "metadata": {},
   "source": [
    "### Custom Middleware: \n",
    "\n",
    "Build custom middleware by implementing hooks that run at specific points in the agent execution flow.\n",
    "Ref: https://docs.langchain.com/oss/python/langchain/middleware/custom\n",
    "\n",
    "#### Decorator-based middleware\n",
    "Quick and simple for single-hook middleware. Use decorators to wrap individual functions.\n",
    "Available decorators:\n",
    "##### Node-style:\n",
    "1. @before_agent - Runs before agent starts (once per invocation)\n",
    "2. @before_model - Runs before each model call\n",
    "3. @after_model - Runs after each model response\n",
    "4. @after_agent - Runs after agent completes (once per invocation)\n",
    "##### Wrap-style:\n",
    "1. @wrap_model_call - Wraps each model call with custom logic\n",
    "2. @wrap_tool_call - Wraps each tool call with custom logic\n",
    "##### Convenience:\n",
    "1. @dynamic_prompt - Generates dynamic system prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d89f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import (\n",
    "    before_model,\n",
    "    wrap_model_call,\n",
    "    AgentState,\n",
    "    ModelRequest,\n",
    "    ModelResponse,\n",
    ")\n",
    "from langchain.agents import create_agent\n",
    "from langgraph.runtime import Runtime\n",
    "from typing import Any, Callable\n",
    "\n",
    "\n",
    "@before_model\n",
    "def log_before_model(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "    print(f\"About to call model with {len(state['messages'])} messages\")\n",
    "    return None\n",
    "\n",
    "@wrap_model_call\n",
    "def retry_model(\n",
    "    request: ModelRequest,\n",
    "    handler: Callable[[ModelRequest], ModelResponse],\n",
    ") -> ModelResponse:\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            return handler(request)\n",
    "        except Exception as e:\n",
    "            if attempt == 2:\n",
    "                raise\n",
    "            print(f\"Retry {attempt + 1}/3 after error: {e}\")\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    middleware=[log_before_model, retry_model],\n",
    "    tools=[...],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf6c942",
   "metadata": {},
   "source": [
    "#### Class-based middleware\n",
    "\n",
    "More powerful for complex middleware with multiple hooks or configuration. Use classes when you need to define both sync and async implementations for the same hook, or when you want to combine multiple hooks in a single middleware.\n",
    "\n",
    "When to use classes:\n",
    "1. Defining both sync and async implementations for the same hook\n",
    "2. Multiple hooks needed in a single middleware\n",
    "3. Complex configuration required (e.g., configurable thresholds, custom models)\n",
    "4. Reuse across projects with init-time configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a89ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import (\n",
    "    AgentMiddleware,\n",
    "    AgentState,\n",
    "    ModelRequest,\n",
    "    ModelResponse,\n",
    ")\n",
    "from langgraph.runtime import Runtime\n",
    "from typing import Any, Callable\n",
    "\n",
    "class LoggingMiddleware(AgentMiddleware):\n",
    "    def before_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "        print(f\"About to call model with {len(state['messages'])} messages\")\n",
    "        return None\n",
    "\n",
    "    def after_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "        print(f\"Model returned: {state['messages'][-1].content}\")\n",
    "        return None\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    middleware=[LoggingMiddleware()],\n",
    "    tools=[...],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a35e36",
   "metadata": {},
   "source": [
    "#### Custom state schema\n",
    "\n",
    "Middleware can extend the agent’s state with custom properties. This enables middleware to:\n",
    "1. Track state across execution: Maintain counters, flags, or other values that persist throughout the agent’s execution lifecycle\n",
    "2. Share data between hooks: Pass information from before_model to after_model or between different middleware instances\n",
    "3. Implement cross-cutting concerns: Add functionality like rate limiting, usage tracking, user context, or audit logging without modifying the core agent logic\n",
    "4. Make conditional decisions: Use accumulated state to determine whether to continue execution, jump to different nodes, or modify behavior dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78bf698",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.messages import HumanMessage\n",
    "from langchain.agents.middleware import AgentState, before_model, after_model\n",
    "from typing_extensions import NotRequired\n",
    "from typing import Any\n",
    "from langgraph.runtime import Runtime\n",
    "\n",
    "\n",
    "class CustomState(AgentState):\n",
    "    model_call_count: NotRequired[int]\n",
    "    user_id: NotRequired[str]\n",
    "\n",
    "\n",
    "@before_model(state_schema=CustomState, can_jump_to=[\"end\"])\n",
    "def check_call_limit(state: CustomState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "    count = state.get(\"model_call_count\", 0)\n",
    "    if count > 10:\n",
    "        return {\"jump_to\": \"end\"}\n",
    "    return None\n",
    "\n",
    "\n",
    "@after_model(state_schema=CustomState)\n",
    "def increment_counter(state: CustomState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "    return {\"model_call_count\": state.get(\"model_call_count\", 0) + 1}\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    middleware=[check_call_limit, increment_counter],\n",
    "    tools=[],\n",
    ")\n",
    "\n",
    "# Invoke with custom state\n",
    "result = agent.invoke({\n",
    "    \"messages\": [HumanMessage(\"Hello\")],\n",
    "    \"model_call_count\": 0,\n",
    "    \"user_id\": \"user-123\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1a555c",
   "metadata": {},
   "source": [
    "#### Agent jumps\n",
    "To exit early from middleware, return a dictionary with jump_to:\n",
    "#### Available jump targets:\n",
    "1. 'end': Jump to the end of the agent execution (or the first after_agent hook)\n",
    "2. 'tools': Jump to the tools node\n",
    "3. 'model': Jump to the model node (or the first before_model hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a83e498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import after_model, hook_config, AgentState\n",
    "from langchain.messages import AIMessage\n",
    "from langgraph.runtime import Runtime\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "@after_model\n",
    "@hook_config(can_jump_to=[\"end\"])\n",
    "def check_for_blocked(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if \"BLOCKED\" in last_message.content:\n",
    "        return {\n",
    "            \"messages\": [AIMessage(\"I cannot respond to that request.\")],\n",
    "            \"jump_to\": \"end\"\n",
    "        }\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a678c0c",
   "metadata": {},
   "source": [
    "Use appropriate hook types:\n",
    "1. Node-style for sequential logic (logging, validation)\n",
    "2. Wrap-style for control flow (retry, fallback, caching)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
