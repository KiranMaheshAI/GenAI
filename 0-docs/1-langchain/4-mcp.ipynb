{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c1708c3",
   "metadata": {},
   "source": [
    "# Model Context Protocol\n",
    "\n",
    "Model Context Protocol (MCP) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the langchain-mcp-adapters library.\n",
    "\n",
    "langchain-mcp-adapters enables agents to use tools defined across one or more MCP servers.\n",
    "\n",
    "MultiServerMCPClient is stateless by default. Each tool invocation creates a fresh MCP ClientSession, executes the tool, and then cleans up. See the stateful sessions section for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc94459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Math server (STDIO transport)\n",
    "\n",
    "# The math server is a simple HTTP server that listens on port 8000 and \n",
    "# responds to GET requests with a JSON object containing a math problem and its solution.\n",
    "from fastmcp import FastMCP\n",
    "\n",
    "mcp = FastMCP(\"Math\")\n",
    "\n",
    "@mcp.tool()\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@mcp.tool()\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers\"\"\"\n",
    "    return a * b\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run(transport=\"stdio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0c74af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The http transport (also referred to as streamable-http) uses HTTP requests for client-server communication\n",
    "# Weather server with Stremable HTTP transport\n",
    "\n",
    "from fastmcp import FastMCP\n",
    "\n",
    "mcp = FastMCP(\"Weather\")\n",
    "\n",
    "@mcp.tool()\n",
    "async def get_weather(location: str) -> str:\n",
    "    \"\"\"Get weather for location.\"\"\"\n",
    "    return \"It's always sunny in New York\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run(transport=\"streamable-http\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524bfb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mcp Client with HTTP transport with header\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"weather\": {\n",
    "            \"transport\": \"http\",\n",
    "            \"url\": \"http://localhost:8000/mcp\",\n",
    "            \"headers\": {  \n",
    "                \"Authorization\": \"Bearer YOUR_TOKEN\",  \n",
    "                \"X-Custom-Header\": \"custom-value\"\n",
    "            },  \n",
    "        }\n",
    "    }\n",
    ")\n",
    "tools = await client.get_tools()\n",
    "agent = create_agent(\"openai:gpt-4.1\", tools)\n",
    "response = await agent.ainvoke({\"messages\": \"what is the weather in nyc?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc7f477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient  \n",
    "from langchain.agents import create_agent\n",
    "\n",
    "\n",
    "client = MultiServerMCPClient(  \n",
    "    {\n",
    "        \"math\": {\n",
    "            \"transport\": \"stdio\",  # Local subprocess communication\n",
    "            \"command\": \"python\",\n",
    "            # Absolute path to your math_server.py file\n",
    "            \"args\": [\"/path/to/math_server.py\"],\n",
    "        },\n",
    "        \"weather\": {\n",
    "            \"transport\": \"http\",  # HTTP-based remote server\n",
    "            # Ensure you start your weather server on port 8000\n",
    "            \"url\": \"http://localhost:8000/mcp\",\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "tools = await client.get_tools()  \n",
    "agent = create_agent(\n",
    "    \"claude-sonnet-4-5-20250929\",\n",
    "    tools  \n",
    ")\n",
    "math_response = await agent.ainvoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what's (3 + 5) x 12?\"}]}\n",
    ")\n",
    "weather_response = await agent.ainvoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in nyc?\"}]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c431b3",
   "metadata": {},
   "source": [
    "Client launches server as a subprocess and communicates via standard input/output. Best for local tools and simple setups.\n",
    "Unlike HTTP transports, stdio connections are inherently stateful—the subprocess persists for the lifetime of the client connection. However, when using MultiServerMCPClient without explicit session management, each tool call still creates a new session.\n",
    "\n",
    "By default, MultiServerMCPClient is stateless—each tool invocation creates a fresh MCP session, executes the tool, and then cleans up. If you need to control the lifecycle of an MCP session (for example, when working with a stateful server that maintains context across tool calls), you can create a persistent ClientSession using client.session()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93b4a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "client = MultiServerMCPClient({...})\n",
    "\n",
    "# Create a session explicitly\n",
    "async with client.session(\"server_name\") as session:  \n",
    "    # Pass the session to load tools, resources, or prompts\n",
    "    tools = await load_mcp_tools(session)  \n",
    "    agent = create_agent(\n",
    "        \"anthropic:claude-3-7-sonnet-latest\",\n",
    "        tools\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d5548c",
   "metadata": {},
   "source": [
    "## Appending structured content via interceptor\n",
    "If you want structured content to be visible in the conversation history (visible to the model), you can use an interceptor to automatically append structured content to the tool result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c7c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain_mcp_adapters.interceptors import MCPToolCallRequest\n",
    "from mcp.types import TextContent\n",
    "\n",
    "async def append_structured_content(request: MCPToolCallRequest, handler):\n",
    "    \"\"\"Append structured content from artifact to tool message.\"\"\"\n",
    "    result = await handler(request)\n",
    "    if result.structuredContent:\n",
    "        result.content += [\n",
    "            TextContent(type=\"text\", text=json.dumps(result.structuredContent)),\n",
    "        ]\n",
    "    return result\n",
    "\n",
    "client = MultiServerMCPClient({...}, tool_interceptors=[append_structured_content])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f6b4a9",
   "metadata": {},
   "source": [
    "## Multimodal tool content\n",
    "\n",
    "MCP tools can return multimodal content (images, text, etc.) in their responses. When an MCP server returns content with multiple parts (e.g., text and images), the adapter converts them to LangChain’s standard content blocks. You can access the standardized representation via the content_blocks property on the ToolMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcc7572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "client = MultiServerMCPClient({...})\n",
    "tools = await client.get_tools()\n",
    "agent = create_agent(\"claude-sonnet-4-5-20250929\", tools)\n",
    "\n",
    "result = await agent.ainvoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Take a screenshot of the current page\"}]}\n",
    ")\n",
    "\n",
    "# Access multimodal content from tool messages\n",
    "for message in result[\"messages\"]:\n",
    "    if message.type == \"tool\":\n",
    "        # Raw content in provider-native format\n",
    "        print(f\"Raw content: {message.content}\")\n",
    "\n",
    "        # Standardized content blocks  #\n",
    "        for block in message.content_blocks:  \n",
    "            if block[\"type\"] == \"text\":  \n",
    "                print(f\"Text: {block['text']}\")  \n",
    "            elif block[\"type\"] == \"image\":  \n",
    "                print(f\"Image URL: {block.get('url')}\")  \n",
    "                print(f\"Image base64: {block.get('base64', '')[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6369f43a",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "Resources allow MCP servers to expose data—such as files, database records, or API responses—that can be read by clients. LangChain converts MCP resources into Blob objects, which provide a unified interface for handling both text and binary content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde12f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "client = MultiServerMCPClient({...})\n",
    "\n",
    "# Load all resources from a server\n",
    "blobs = await client.get_resources(\"server_name\")  \n",
    "\n",
    "# Or load specific resources by URI\n",
    "blobs = await client.get_resources(\"server_name\", uris=[\"file:///path/to/file.txt\"])  \n",
    "\n",
    "for blob in blobs:\n",
    "    print(f\"URI: {blob.metadata['uri']}, MIME type: {blob.mimetype}\")\n",
    "    print(blob.as_string())  # For text content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4186e6e5",
   "metadata": {},
   "source": [
    "You can also use load_mcp_resources directly with a session for more control:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6a2bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain_mcp_adapters.resources import load_mcp_resources\n",
    "\n",
    "client = MultiServerMCPClient({...})\n",
    "\n",
    "async with client.session(\"server_name\") as session:\n",
    "    # Load all resources\n",
    "    blobs = await load_mcp_resources(session)\n",
    "\n",
    "    # Or load specific resources by URI\n",
    "    blobs = await load_mcp_resources(session, uris=[\"file:///path/to/file.txt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015fa0af",
   "metadata": {},
   "source": [
    "## Prompts\n",
    "\n",
    "Prompts allow MCP servers to expose reusable prompt templates that can be retrieved and used by clients. LangChain converts MCP prompts into messages, making them easy to integrate into chat-based workflows.\n",
    "Use client.get_prompt() to load a prompt from an MCP server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205a84f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "client = MultiServerMCPClient({...})\n",
    "\n",
    "# Load a prompt by name\n",
    "messages = await client.get_prompt(\"server_name\", \"summarize\")  \n",
    "\n",
    "# Load a prompt with arguments\n",
    "messages = await client.get_prompt(  \n",
    "    \"server_name\",  \n",
    "    \"code_review\",  \n",
    "    arguments={\"language\": \"python\", \"focus\": \"security\"}  \n",
    ")  \n",
    "\n",
    "# Use the messages in your workflow\n",
    "for message in messages:\n",
    "    print(f\"{message.type}: {message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cde434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use load_mcp_prompt directly with a session for more control:\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain_mcp_adapters.prompts import load_mcp_prompt\n",
    "\n",
    "client = MultiServerMCPClient({...})\n",
    "\n",
    "async with client.session(\"server_name\") as session:\n",
    "    # Load a prompt by name\n",
    "    messages = await load_mcp_prompt(session, \"summarize\")\n",
    "\n",
    "    # Load a prompt with arguments\n",
    "    messages = await load_mcp_prompt(\n",
    "        session,\n",
    "        \"code_review\",\n",
    "        arguments={\"language\": \"python\", \"focus\": \"security\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b60dea5",
   "metadata": {},
   "source": [
    "# Tool Interceptors\n",
    "\n",
    "MCP servers run as separate processes—they can’t access LangGraph runtime information like the store, context, or agent state. Interceptors bridge this gap by giving you access to this runtime context during MCP tool execution.\n",
    "\n",
    "Interceptors also provide middleware-like control over tool calls: you can modify requests, implement retries, add headers dynamically, or short-circuit execution entirely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9007a19",
   "metadata": {},
   "source": [
    "## Accessing runtime context\n",
    "When MCP tools are used within a LangChain agent (via create_agent), interceptors receive access to the ToolRuntime context. This provides access to the tool call ID, state, config, and store—enabling powerful patterns for accessing user data, persisting information, and controlling agent behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbbfbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain_mcp_adapters.interceptors import MCPToolCallRequest\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "@dataclass\n",
    "class Context:\n",
    "    user_id: str\n",
    "    api_key: str\n",
    "\n",
    "async def inject_user_context(\n",
    "    request: MCPToolCallRequest,\n",
    "    handler,\n",
    "):\n",
    "    \"\"\"Inject user credentials into MCP tool calls.\"\"\"\n",
    "    runtime = request.runtime\n",
    "    # context\n",
    "    user_id = runtime.context.user_id  \n",
    "    api_key = runtime.context.api_key  \n",
    "\n",
    "    # store\n",
    "    store = runtime.store\n",
    "    prefs = store.get((\"preferences\",), user_id)  \n",
    "\n",
    "    # state\n",
    "    state = runtime.state  \n",
    "    is_authenticated = state.get(\"authenticated\", False)  \n",
    "\n",
    "    # tool call id\n",
    "    tool_call_id = runtime.tool_call_id  \n",
    "\n",
    "    # Add user context to tool arguments\n",
    "    modified_request = request.override(\n",
    "        args={**request.args, \"user_id\": user_id}\n",
    "    )\n",
    "    return await handler(modified_request)\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {...},\n",
    "    tool_interceptors=[inject_user_context],\n",
    ")\n",
    "tools = await client.get_tools()\n",
    "agent = create_agent(\"gpt-4o\", tools, context_schema=Context)\n",
    "\n",
    "# Invoke with user context\n",
    "result = await agent.ainvoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Search my orders\"}]},\n",
    "    context={\"user_id\": \"user_123\", \"api_key\": \"sk-...\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc74139",
   "metadata": {},
   "source": [
    "## State updates and commands\n",
    "\n",
    "Interceptors can return Command objects to update agent state or control graph execution flow. This is useful for tracking task progress, switching between agents, or ending execution early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0865bf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentState, create_agent\n",
    "from langchain_mcp_adapters.interceptors import MCPToolCallRequest\n",
    "from langchain.messages import ToolMessage\n",
    "from langgraph.types import Command\n",
    "\n",
    "async def handle_task_completion(\n",
    "    request: MCPToolCallRequest,\n",
    "    handler,\n",
    "):\n",
    "    \"\"\"Mark task complete and hand off to summary agent.\"\"\"\n",
    "    result = await handler(request)\n",
    "\n",
    "    if request.name == \"submit_order\":\n",
    "        return Command(\n",
    "            update={\n",
    "                \"messages\": [result] if isinstance(result, ToolMessage) else [],\n",
    "                \"task_status\": \"completed\",  \n",
    "            },\n",
    "            goto=\"summary_agent\",  \n",
    "        )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cdeb72",
   "metadata": {},
   "source": [
    "# Custom interceptors\n",
    "\n",
    "Interceptors are async functions that wrap tool execution, enabling request/response modification, retry logic, and other cross-cutting concerns. They follow an “onion” pattern where the first interceptor in the list is the outermost layer.\n",
    "\n",
    "An interceptor is an async function that receives a request and a handler. You can modify the request before calling the handler, modify the response after, or skip the handler entirely\n",
    "\n",
    "Use request.override() to create a modified request. This follows an immutable pattern, leaving the original request unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5283e243",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def double_args_interceptor(\n",
    "    request: MCPToolCallRequest,\n",
    "    handler,\n",
    "):\n",
    "    \"\"\"Double all numeric arguments before execution.\"\"\"\n",
    "    modified_args = {k: v * 2 for k, v in request.args.items()}\n",
    "    modified_request = request.override(args=modified_args)  \n",
    "    return await handler(modified_request)\n",
    "\n",
    "# Original call: add(a=2, b=3) becomes add(a=4, b=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29cb581",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def outer_interceptor(request, handler):\n",
    "    print(\"outer: before\")\n",
    "    result = await handler(request)\n",
    "    print(\"outer: after\")\n",
    "    return result\n",
    "\n",
    "async def inner_interceptor(request, handler):\n",
    "    print(\"inner: before\")\n",
    "    result = await handler(request)\n",
    "    print(\"inner: after\")\n",
    "    return result\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {...},\n",
    "    tool_interceptors=[outer_interceptor, inner_interceptor],  \n",
    ")\n",
    "\n",
    "# Execution order:\n",
    "# outer: before -> inner: before -> tool execution -> inner: after -> outer: after"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f822aa8",
   "metadata": {},
   "source": [
    "## Progress notifications\n",
    "\n",
    "Subscribe to progress updates for long-running tool executions.\n",
    "\n",
    "The CallbackContext provides:\n",
    "1. server_name: Name of the MCP server\n",
    "2. tool_name: Name of the tool being executed (available during tool calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238af4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain_mcp_adapters.callbacks import Callbacks, CallbackContext\n",
    "\n",
    "async def on_progress(\n",
    "    progress: float,\n",
    "    total: float | None,\n",
    "    message: str | None,\n",
    "    context: CallbackContext,\n",
    "):\n",
    "    \"\"\"Handle progress updates from MCP servers.\"\"\"\n",
    "    percent = (progress / total * 100) if total else progress\n",
    "    tool_info = f\" ({context.tool_name})\" if context.tool_name else \"\"\n",
    "    print(f\"[{context.server_name}{tool_info}] Progress: {percent:.1f}% - {message}\")\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {...},\n",
    "    callbacks=Callbacks(on_progress=on_progress),  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e141cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MCP protocol supports logging notifications from servers. Use the Callbacks class to subscribe to these events.\n",
    "\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain_mcp_adapters.callbacks import Callbacks, CallbackContext\n",
    "from mcp.types import LoggingMessageNotificationParams\n",
    "\n",
    "async def on_logging_message(\n",
    "    params: LoggingMessageNotificationParams,\n",
    "    context: CallbackContext,\n",
    "):\n",
    "    \"\"\"Handle log messages from MCP servers.\"\"\"\n",
    "    print(f\"[{context.server_name}] {params.level}: {params.data}\")\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {...},\n",
    "    callbacks=Callbacks(on_logging_message=on_logging_message),  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f8e473",
   "metadata": {},
   "source": [
    "# Elicitation\n",
    "Elicitation allows MCP servers to request additional input from users during tool execution. Instead of requiring all inputs upfront, servers can interactively ask for information as needed.\n",
    "​\n",
    "The elicitation callback can return one of three actions:\n",
    "1. accept -> User provided valid input. Include the data in the content field. \n",
    "2. decline -> User chose not to provide the requested information.-> ElicitResult(action=\"decline\")\n",
    "3. cancel -> User cancelled the operation entirely. -> ElicitResult(action=\"cancel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119568dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from mcp.server.fastmcp import Context, FastMCP\n",
    "\n",
    "server = FastMCP(\"Profile\")\n",
    "\n",
    "class UserDetails(BaseModel):\n",
    "    email: str\n",
    "    age: int\n",
    "\n",
    "@server.tool()\n",
    "async def create_profile(name: str, ctx: Context) -> str:\n",
    "    \"\"\"Create a user profile, requesting details via elicitation.\"\"\"\n",
    "    result = await ctx.elicit(  \n",
    "        message=f\"Please provide details for {name}'s profile:\",  \n",
    "        schema=UserDetails,  \n",
    "    )  \n",
    "    if result.action == \"accept\" and result.data:\n",
    "        return f\"Created profile for {name}: email={result.data.email}, age={result.data.age}\"\n",
    "    if result.action == \"decline\":\n",
    "        return f\"User declined. Created minimal profile for {name}.\"\n",
    "    return \"Profile creation cancelled.\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    server.run(transport=\"http\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeff013",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain_mcp_adapters.callbacks import Callbacks, CallbackContext\n",
    "from mcp.shared.context import RequestContext\n",
    "from mcp.types import ElicitRequestParams, ElicitResult\n",
    "\n",
    "async def on_elicitation(\n",
    "    mcp_context: RequestContext,\n",
    "    params: ElicitRequestParams,\n",
    "    context: CallbackContext,\n",
    ") -> ElicitResult:\n",
    "    \"\"\"Handle elicitation requests from MCP servers.\"\"\"\n",
    "    # In a real application, you would prompt the user for input\n",
    "    # based on params.message and params.requestedSchema\n",
    "    return ElicitResult(  \n",
    "        action=\"accept\",  \n",
    "        content={\"email\": \"user@example.com\", \"age\": 25},  \n",
    "    )  \n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"profile\": {\n",
    "            \"url\": \"http://localhost:8000/mcp\",\n",
    "            \"transport\": \"http\",\n",
    "        }\n",
    "    },\n",
    "    callbacks=Callbacks(on_elicitation=on_elicitation),  \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
