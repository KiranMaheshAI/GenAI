{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7558315",
   "metadata": {},
   "source": [
    "# Persistance\n",
    "\n",
    "LangGraph has a built-in persistence layer, implemented through checkpointers. When you compile a graph with a checkpointer, the checkpointer saves a checkpoint of the graph state at every super-step. Those checkpoints are saved to a thread, which can be accessed after graph execution. Because threads allow access to graph‚Äôs state after execution, several powerful capabilities including human-in-the-loop, memory, time travel, and fault-tolerance are all possible.\n",
    "\n",
    "A thread is a unique ID or thread identifier assigned to each checkpoint saved by a checkpointer. It contains the accumulated state of a sequence of runs. When a run is executed, the state of the underlying graph of the assistant will be persisted to the thread.\n",
    "\n",
    "{\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "The checkpointer uses thread_id as the primary key for storing and retrieving checkpoints. Without it, the checkpointer cannot save state or resume execution after an interrupt, since the checkpointer uses thread_id to load the saved state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de7b737",
   "metadata": {},
   "source": [
    "#### Checkpoints\n",
    "The state of a thread at a particular point in time is called a checkpoint. Checkpoint is a snapshot of the graph state saved at each super-step and is represented by StateSnapshot.\n",
    "object with the following key properties:\n",
    "1. config: Config associated with this checkpoint.\n",
    "2. metadata: Metadata associated with this checkpoint.\n",
    "3. values: Values of the state channels at this point in time.\n",
    "4. next: A tuple of the node names to execute next in the graph.\n",
    "5. tasks: A tuple of PregelTask objects that contain information about next tasks to be executed. If the step was previously attempted, it will include error information. If a graph was interrupted dynamically from within a node, tasks will contain additional data associated with interrupts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57e9cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from operator import add\n",
    "\n",
    "class State(TypedDict):\n",
    "    foo: str\n",
    "    bar: Annotated[list[str], add]\n",
    "\n",
    "def node_a(state: State):\n",
    "    return {\"foo\": \"a\", \"bar\": [\"a\"]}\n",
    "\n",
    "def node_b(state: State):\n",
    "    return {\"foo\": \"b\", \"bar\": [\"b\"]}\n",
    "\n",
    "\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(node_a)\n",
    "workflow.add_node(node_b)\n",
    "workflow.add_edge(START, \"node_a\")\n",
    "workflow.add_edge(\"node_a\", \"node_b\")\n",
    "workflow.add_edge(\"node_b\", END)\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "graph = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "config: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "graph.invoke({\"foo\": \"\", \"bar\":[]}, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af2901b",
   "metadata": {},
   "source": [
    "### Get state\n",
    "When interacting with the saved graph state, you must specify a thread identifier. You can view the latest state of the graph by calling graph.get_state(config). This will return a StateSnapshot object that corresponds to the latest checkpoint associated with the thread ID provided in the config or a checkpoint associated with a checkpoint ID for the thread, if provided.\n",
    "\n",
    "You can get the full history of the graph execution for a given thread by calling graph.get_state_history(config). This will return a list of StateSnapshot objects associated with the thread ID provided in the config. Importantly, the checkpoints will be ordered chronologically with the most recent checkpoint / StateSnapshot being the first in the list.\n",
    "\n",
    "### Replay\n",
    "It‚Äôs also possible to play-back a prior graph execution. If we invoke a graph with a thread_id and a checkpoint_id, then we will re-play the previously executed steps before a checkpoint that corresponds to the checkpoint_id, and only execute the steps after the checkpoint.\n",
    "thread_id is the ID of a thread.\n",
    "checkpoint_id is an identifier that refers to a specific checkpoint within a thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb1e758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the latest state snapshot\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "graph.get_state(config)\n",
    "\n",
    "# get a state snapshot for a specific checkpoint_id\n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"checkpoint_id\": \"1ef663ba-28fe-6528-8002-5a559208592c\"}}\n",
    "graph.get_state(config)\n",
    "\n",
    "# get state history\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "list(graph.get_state_history(config))\n",
    "\n",
    "# replay :\n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"checkpoint_id\": \"0c62ca34-ac19-445d-bbb0-5b4984975b2a\"}}\n",
    "graph.invoke(None, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697b0084",
   "metadata": {},
   "source": [
    "### Memory Store\n",
    "\n",
    "With checkpointers alone, we cannot share information across threads. This motivates the need for the Store interface. As an illustration, we can define an InMemoryStore to store information about a user across threads. We simply compile our graph with a checkpointer, as before, and with our new in_memory_store variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1984c204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.store.memory import InMemoryStore\n",
    "import uuid\n",
    "\n",
    "in_memory_store = InMemoryStore()\n",
    "user_id = \"1\"\n",
    "namespace_for_memory = (user_id, \"memories\")\n",
    "memory_id = str(uuid.uuid4())\n",
    "memory = {\"food_preference\" : \"I like pizza\"}\n",
    "in_memory_store.put(namespace_for_memory, memory_id, memory)\n",
    "memories = in_memory_store.search(namespace_for_memory)\n",
    "memories[-1].dict()\n",
    "# {'value': {'food_preference': 'I like pizza'},\n",
    "#  'key': '07e0caf4-1631-47b7-b15f-65515d4c1843',\n",
    "#  'namespace': ['1', 'memories'],\n",
    "#  'created_at': '2024-10-02T17:22:31.590602+00:00',\n",
    "#  'updated_at': '2024-10-02T17:22:31.590605+00:00'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae09fdcc",
   "metadata": {},
   "source": [
    "### Semantic Search\n",
    "Beyond simple retrieval, the store also supports semantic search, allowing you to find memories based on meaning rather than exact matches. To enable this, configure the store with an embedding model. You can control which parts of your memories get embedded by configuring the fields parameter or by specifying the index parameter when storing memories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d4c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import init_embeddings\n",
    "\n",
    "store = InMemoryStore(\n",
    "    index={\n",
    "        \"embed\": init_embeddings(\"openai:text-embedding-3-small\"),  # Embedding provider\n",
    "        \"dims\": 1536,                              # Embedding dimensions\n",
    "        \"fields\": [\"food_preference\", \"$\"]              # Fields to embed\n",
    "    }\n",
    ")\n",
    "\n",
    "# Find memories about food preferences\n",
    "# (This can be done after putting memories into the store)\n",
    "memories = store.search(\n",
    "    namespace_for_memory,\n",
    "    query=\"What does the user like to eat?\",\n",
    "    limit=3  # Return top 3 matches\n",
    ")\n",
    "\n",
    "# Store with specific fields to embed\n",
    "store.put(\n",
    "    namespace_for_memory,\n",
    "    str(uuid.uuid4()),\n",
    "    {\n",
    "        \"food_preference\": \"I love Italian cuisine\",\n",
    "        \"context\": \"Discussing dinner plans\"\n",
    "    },\n",
    "    index=[\"food_preference\"]  # Only embed \"food_preferences\" field\n",
    ")\n",
    "\n",
    "# Store without embedding (still retrievable, but not searchable)\n",
    "store.put(\n",
    "    namespace_for_memory,\n",
    "    str(uuid.uuid4()),\n",
    "    {\"system_info\": \"Last updated: 2024-01-01\"},\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4c8040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "# We need this because we want to enable threads (conversations)\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "# ... Define the graph ...\n",
    "\n",
    "# Compile the graph with the checkpointer and store\n",
    "graph = graph.compile(checkpointer=checkpointer, store=in_memory_store)\n",
    "\n",
    "# Invoke the graph\n",
    "user_id = \"1\"\n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": user_id}}\n",
    "\n",
    "# First let's just say hi to the AI\n",
    "for update in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"hi\"}]}, config, stream_mode=\"updates\"\n",
    "):\n",
    "    print(update)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d04554",
   "metadata": {},
   "source": [
    "We can access the in_memory_store and the user_id in any node by passing store: BaseStore and config: RunnableConfig as node arguments. Here‚Äôs how we might use semantic search in a node to find relevant memories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9816a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_memory(state: MessagesState, config: RunnableConfig, *, store: BaseStore):\n",
    "\n",
    "    # Get the user id from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Namespace the memory\n",
    "    namespace = (user_id, \"memories\")\n",
    "\n",
    "    # ... Analyze conversation and create a new memory\n",
    "\n",
    "    # Create a new memory ID\n",
    "    memory_id = str(uuid.uuid4())\n",
    "\n",
    "    # We create a new memory\n",
    "    store.put(namespace, memory_id, {\"memory\": memory})\n",
    "    # Search based on the most recent message\n",
    "    memories = store.search(\n",
    "        namespace,\n",
    "        query=state[\"messages\"][-1].content,\n",
    "        limit=3\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee923936",
   "metadata": {},
   "source": [
    "LangGraph provides several checkpointer implementations, all implemented via standalone, installable libraries:\n",
    "\n",
    "1. langgraph-checkpoint: The base interface for checkpointer savers (BaseCheckpointSaver) and serialization/deserialization interface (SerializerProtocol). Includes in-memory checkpointer implementation (InMemorySaver) for experimentation. LangGraph comes with langgraph-checkpoint included.\n",
    "2. langgraph-checkpoint-sqlite: An implementation of LangGraph checkpointer that uses SQLite database (SqliteSaver / AsyncSqliteSaver). Ideal for experimentation and local workflows. Needs to be installed separately.\n",
    "3. langgraph-checkpoint-postgres: An advanced checkpointer that uses Postgres database (PostgresSaver / AsyncPostgresSaver), used in LangSmith. Ideal for using in production. Needs to be installed separately.\n",
    "\n",
    "Checkpointer interface\n",
    "Each checkpointer conforms to BaseCheckpointSaver interface and implements the following methods:\n",
    "1. .put - Store a checkpoint with its configuration and metadata.\n",
    "2. .put_writes - Store intermediate writes linked to a checkpoint (i.e. pending writes).\n",
    "3. .get_tuple - Fetch a checkpoint tuple using for a given configuration (thread_id and checkpoint_id). This is used to populate StateSnapshot in graph.get_state().\n",
    "4. .list - List checkpoints that match a given configuration and filter criteria. This is used to populate state history in graph.get_state_history()\n",
    "If the checkpointer is used with asynchronous graph execution (i.e. executing the graph via .ainvoke, .astream, .abatch), asynchronous versions of the above methods will be used (.aput, .aput_writes, .aget_tuple, .alist)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d014e5",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "\n",
    "LangGraph implements a streaming system to surface real-time updates. Streaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.\n",
    "\n",
    "Supported stream modes:\n",
    "1. values: Streams the full value of the state after each step of the graph.\n",
    "2. updates: Streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g., multiple nodes are run), those updates are streamed separately.\n",
    "3. custom: Streams custom data from inside your graph nodes.\n",
    "4. messages: Streams 2-tuples (LLM token, metadata) from any graph nodes where an LLM is invoked.\n",
    "5. debug: Streams as much information as possible throughout the execution of the graph.\n",
    "\n",
    "Use the stream modes updates and values to stream the state of the graph as it executes.\n",
    "1. updates streams the updates to the state after each step of the graph.\n",
    "2. values streams the full value of the state after each step of the graph.\n",
    "\n",
    "### Stream Subgraph Outputs:\n",
    "To include outputs from subgraphs in the streamed outputs, you can set subgraphs=True in the .stream() method of the parent graph. This will stream outputs from both the parent graph and any subgraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca80888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\"foo\": \"foo\"},\n",
    "    # Set subgraphs=True to stream outputs from subgraphs\n",
    "    subgraphs=True,  \n",
    "    stream_mode=\"updates\",\n",
    "):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb8987e",
   "metadata": {},
   "source": [
    "To stream tokens only from specific nodes, use stream_mode=\"messages\" and filter the outputs by the langgraph_node field in the streamed metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b433e2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "      topic: str\n",
    "      joke: str\n",
    "      poem: str\n",
    "\n",
    "\n",
    "def write_joke(state: State):\n",
    "      topic = state[\"topic\"]\n",
    "      joke_response = model.invoke(\n",
    "            [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}]\n",
    "      )\n",
    "      return {\"joke\": joke_response.content}\n",
    "\n",
    "\n",
    "def write_poem(state: State):\n",
    "      topic = state[\"topic\"]\n",
    "      poem_response = model.invoke(\n",
    "            [{\"role\": \"user\", \"content\": f\"Write a short poem about {topic}\"}]\n",
    "      )\n",
    "      return {\"poem\": poem_response.content}\n",
    "\n",
    "\n",
    "graph = (\n",
    "      StateGraph(State)\n",
    "      .add_node(write_joke)\n",
    "      .add_node(write_poem)\n",
    "      # write both the joke and the poem concurrently\n",
    "      .add_edge(START, \"write_joke\")\n",
    "      .add_edge(START, \"write_poem\")\n",
    "      .compile()\n",
    ")\n",
    "\n",
    "# The \"messages\" stream mode returns a tuple of (message_chunk, metadata)\n",
    "# where message_chunk is the token streamed by the LLM and metadata is a dictionary\n",
    "# with information about the graph node where the LLM was called and other information\n",
    "for msg, metadata in graph.stream(\n",
    "    {\"topic\": \"cats\"},\n",
    "    stream_mode=\"messages\",  \n",
    "):\n",
    "    # Filter the streamed tokens by the langgraph_node field in the metadata\n",
    "    # to only include the tokens from the write_poem node\n",
    "    if msg.content and metadata[\"langgraph_node\"] == \"write_poem\":\n",
    "        print(msg.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52ebe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# async LLM call with manual config\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(model=\"gpt-4o-mini\")\n",
    "\n",
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    joke: str\n",
    "\n",
    "# Accept config as an argument in the async node function\n",
    "async def call_model(state, config):\n",
    "    topic = state[\"topic\"]\n",
    "    print(\"Generating joke...\")\n",
    "    # Pass config to model.ainvoke() to ensure proper context propagation\n",
    "    joke_response = await model.ainvoke(  \n",
    "        [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}],\n",
    "        config,\n",
    "    )\n",
    "    return {\"joke\": joke_response.content}\n",
    "\n",
    "graph = (\n",
    "    StateGraph(State)\n",
    "    .add_node(call_model)\n",
    "    .add_edge(START, \"call_model\")\n",
    "    .compile()\n",
    ")\n",
    "\n",
    "# Set stream_mode=\"messages\" to stream LLM tokens\n",
    "async for chunk, metadata in graph.astream(\n",
    "    {\"topic\": \"ice cream\"},\n",
    "    stream_mode=\"messages\",  \n",
    "):\n",
    "    if chunk.content:\n",
    "        print(chunk.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0296ce35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# async custom streaming with stream writer\n",
    "from typing import TypedDict\n",
    "from langgraph.types import StreamWriter\n",
    "\n",
    "class State(TypedDict):\n",
    "      topic: str\n",
    "      joke: str\n",
    "\n",
    "# Add writer as an argument in the function signature of the async node or tool\n",
    "# LangGraph will automatically pass the stream writer to the function\n",
    "async def generate_joke(state: State, writer: StreamWriter):  \n",
    "      writer({\"custom_key\": \"Streaming custom data while generating a joke\"})\n",
    "      return {\"joke\": f\"This is a joke about {state['topic']}\"}\n",
    "\n",
    "graph = (\n",
    "      StateGraph(State)\n",
    "      .add_node(generate_joke)\n",
    "      .add_edge(START, \"generate_joke\")\n",
    "      .compile()\n",
    ")\n",
    "\n",
    "# Set stream_mode=\"custom\" to receive the custom data in the stream  #\n",
    "async for chunk in graph.astream(\n",
    "      {\"topic\": \"ice cream\"},\n",
    "      stream_mode=\"custom\",\n",
    "):\n",
    "      print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69452ec",
   "metadata": {},
   "source": [
    "## Interrupts\n",
    "\n",
    "Interrupts allow you to pause graph execution at specific points and wait for external input before continuing. This enables human-in-the-loop patterns where you need external input to proceed. When an interrupt is triggered, LangGraph saves the graph state using its persistence layer and waits indefinitely until you resume execution.\n",
    "\n",
    "Interrupts work by calling the interrupt() function at any point in your graph nodes. The function accepts any JSON-serializable value which is surfaced to the caller. When you‚Äôre ready to continue, you resume execution by re-invoking the graph using Command, which then becomes the return value of the interrupt() call from inside the node.\n",
    "\n",
    "To use interrupt, you need:\n",
    "1. A checkpointer to persist the graph state (use a durable checkpointer in production)\n",
    "2. A thread ID in your config so the runtime knows which state to resume from\n",
    "3. To call interrupt() where you want to pause (payload must be JSON-serializable)\n",
    "\n",
    "When you call interrupt, here‚Äôs what happens:\n",
    "1. Graph execution gets suspended at the exact point where interrupt is called\n",
    "2. State is saved using the checkpointer so execution can be resumed later, In production, this should be a persistent checkpointer (e.g. backed by a database)\n",
    "3. Value is returned to the caller under __interrupt__; it can be any JSON-serializable value (string, object, array, etc.)\n",
    "4. Graph waits indefinitely until you resume execution with a response\n",
    "5. Response is passed back into the node when you resume, becoming the return value of the interrupt() call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1320c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import interrupt\n",
    "\n",
    "def approval_node(state: State):\n",
    "    # Pause and ask for approval\n",
    "    approved = interrupt(\"Do you approve this action?\")\n",
    "\n",
    "    # When you resume, Command(resume=...) returns that value here\n",
    "    return {\"approved\": approved}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c187d74",
   "metadata": {},
   "source": [
    "### Resuming Interrupts\n",
    "\n",
    "After an interrupt pauses execution, you resume the graph by invoking it again with a Command that contains the resume value. The resume value is passed back to the interrupt call, allowing the node to continue execution with the external input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33dcb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import Command\n",
    "\n",
    "# Initial run - hits the interrupt and pauses\n",
    "# thread_id is the persistent pointer (stores a stable ID in production)\n",
    "config = {\"configurable\": {\"thread_id\": \"thread-1\"}}\n",
    "result = graph.invoke({\"input\": \"data\"}, config=config)\n",
    "\n",
    "# Check what was interrupted\n",
    "# __interrupt__ contains the payload that was passed to interrupt()\n",
    "print(result[\"__interrupt__\"])\n",
    "# > [Interrupt(value='Do you approve this action?')]\n",
    "\n",
    "# Resume with the human's response\n",
    "# The resume payload becomes the return value of interrupt() inside the node\n",
    "graph.invoke(Command(resume=True), config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67604f77",
   "metadata": {},
   "source": [
    "## Common Patterns\n",
    "\n",
    "The key thing that interrupts unlock is the ability to pause execution and wait for external input. This is useful for a variety of use cases, including:\n",
    "\n",
    "### Approval workflows: \n",
    "Pause before executing critical actions (API calls, database changes, financial transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98125808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langgraph.types import interrupt, Command\n",
    "\n",
    "def approval_node(state: State) -> Command[Literal[\"proceed\", \"cancel\"]]:\n",
    "    # Pause execution; payload shows up under result[\"__interrupt__\"]\n",
    "    is_approved = interrupt({\n",
    "        \"question\": \"Do you want to proceed with this action?\",\n",
    "        \"details\": state[\"action_details\"]\n",
    "    })\n",
    "\n",
    "    # Route based on the response\n",
    "    if is_approved:\n",
    "        return Command(goto=\"proceed\")  # Runs after the resume payload is provided\n",
    "    else:\n",
    "        return Command(goto=\"cancel\")\n",
    "\n",
    "\n",
    "# To approve\n",
    "graph.invoke(Command(resume=True), config=config)\n",
    "\n",
    "# To reject\n",
    "graph.invoke(Command(resume=False), config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c752cd46",
   "metadata": {},
   "source": [
    "## Review and edit state\n",
    "\n",
    "Sometimes you want to let a human review and edit part of the graph state before continuing. This is useful for correcting LLMs, adding missing information, or making adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2aa1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import interrupt\n",
    "\n",
    "def review_node(state: State):\n",
    "    # Pause and show the current content for review (surfaces in result[\"__interrupt__\"])\n",
    "    edited_content = interrupt({\n",
    "        \"instruction\": \"Review and edit this content\",\n",
    "        \"content\": state[\"generated_text\"]\n",
    "    })\n",
    "\n",
    "    # Update the state with the edited version\n",
    "    return {\"generated_text\": edited_content}\n",
    "\n",
    "graph.invoke(\n",
    "    Command(resume=\"The edited and improved text\"),  # Value becomes the return from interrupt()\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead2ae12",
   "metadata": {},
   "source": [
    "### Interrupts in tools\n",
    "\n",
    "You can also place interrupts directly inside tool functions. This makes the tool itself pause for approval whenever it‚Äôs called, and allows for human review and editing of the tool call before it is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469cfd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from typing import TypedDict\n",
    "\n",
    "from langchain.tools import tool\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Command, interrupt\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: list[dict]\n",
    "\n",
    "\n",
    "@tool\n",
    "def send_email(to: str, subject: str, body: str):\n",
    "    \"\"\"Send an email to a recipient.\"\"\"\n",
    "\n",
    "    # Pause before sending; payload surfaces in result[\"__interrupt__\"]\n",
    "    response = interrupt({\n",
    "        \"action\": \"send_email\",\n",
    "        \"to\": to,\n",
    "        \"subject\": subject,\n",
    "        \"body\": body,\n",
    "        \"message\": \"Approve sending this email?\",\n",
    "    })\n",
    "\n",
    "    if response.get(\"action\") == \"approve\":\n",
    "        final_to = response.get(\"to\", to)\n",
    "        final_subject = response.get(\"subject\", subject)\n",
    "        final_body = response.get(\"body\", body)\n",
    "\n",
    "        # Actually send the email (your implementation here)\n",
    "        print(f\"[send_email] to={final_to} subject={final_subject} body={final_body}\")\n",
    "        return f\"Email sent to {final_to}\"\n",
    "\n",
    "    return \"Email cancelled by user\"\n",
    "\n",
    "\n",
    "model = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\").bind_tools([send_email])\n",
    "\n",
    "\n",
    "def agent_node(state: AgentState):\n",
    "    # LLM may decide to call the tool; interrupt pauses before sending\n",
    "    result = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": state[\"messages\"] + [result]}\n",
    "\n",
    "\n",
    "builder = StateGraph(AgentState)\n",
    "builder.add_node(\"agent\", agent_node)\n",
    "builder.add_edge(START, \"agent\")\n",
    "builder.add_edge(\"agent\", END)\n",
    "\n",
    "checkpointer = SqliteSaver(sqlite3.connect(\"tool-approval.db\"))\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"email-workflow\"}}\n",
    "initial = graph.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Send an email to alice@example.com about the meeting\"}\n",
    "        ]\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "print(initial[\"__interrupt__\"])  # -> [Interrupt(value={'action': 'send_email', ...})]\n",
    "\n",
    "# Resume with approval and optionally edited arguments\n",
    "resumed = graph.invoke(\n",
    "    Command(resume={\"action\": \"approve\", \"subject\": \"Updated subject\"}),\n",
    "    config=config,\n",
    ")\n",
    "print(resumed[\"messages\"][-1])  # -> Tool result returned by send_email"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca29fe76",
   "metadata": {},
   "source": [
    "### Validating human input\n",
    "\n",
    "Sometimes you need to validate input from humans and ask again if it‚Äôs invalid. You can do this using multiple interrupt calls in a loop.\n",
    "Each time you resume the graph with invalid input, it will ask again with a clearer message. Once valid input is provided, the node completes and the graph continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b945c554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from typing import TypedDict\n",
    "\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Command, interrupt\n",
    "\n",
    "\n",
    "class FormState(TypedDict):\n",
    "    age: int | None\n",
    "\n",
    "\n",
    "def get_age_node(state: FormState):\n",
    "    prompt = \"What is your age?\"\n",
    "\n",
    "    while True:\n",
    "        answer = interrupt(prompt)  # payload surfaces in result[\"__interrupt__\"]\n",
    "\n",
    "        if isinstance(answer, int) and answer > 0:\n",
    "            return {\"age\": answer}\n",
    "\n",
    "        prompt = f\"'{answer}' is not a valid age. Please enter a positive number.\"\n",
    "\n",
    "\n",
    "builder = StateGraph(FormState)\n",
    "builder.add_node(\"collect_age\", get_age_node)\n",
    "builder.add_edge(START, \"collect_age\")\n",
    "builder.add_edge(\"collect_age\", END)\n",
    "\n",
    "checkpointer = SqliteSaver(sqlite3.connect(\"forms.db\"))\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"form-1\"}}\n",
    "first = graph.invoke({\"age\": None}, config=config)\n",
    "print(first[\"__interrupt__\"])  # -> [Interrupt(value='What is your age?', ...)]\n",
    "\n",
    "# Provide invalid data; the node re-prompts\n",
    "retry = graph.invoke(Command(resume=\"thirty\"), config=config)\n",
    "print(retry[\"__interrupt__\"])  # -> [Interrupt(value=\"'thirty' is not a valid age...\", ...)]\n",
    "\n",
    "# Provide valid data; loop exits and state updates\n",
    "final = graph.invoke(Command(resume=30), config=config)\n",
    "print(final[\"age\"])  # -> 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7795ebd",
   "metadata": {},
   "source": [
    "Because interrupts work by re-running the nodes they were called from, side effects called before interrupt should (ideally) be idempotent. For context, idempotency means that the same operation can be applied multiple times without changing the result beyond the initial execution.\n",
    "\n",
    "As an example, you might have an API call to update a record inside of a node. If interrupt is called after that call is made, it will be re-run multiple times when the node is resumed, potentially overwriting the initial update or creating duplicate records.\n",
    "1. ‚úÖ Use idempotent operations before interrupt\n",
    "2. ‚úÖ Place side effects after interrupt calls\n",
    "3. ‚úÖ Separate side effects into separate nodes when possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29112db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approval_node(state: State):\n",
    "    # ‚úÖ Good: only handling the interrupt in this node\n",
    "    approved = interrupt(\"Approve this change?\")\n",
    "\n",
    "    return {\"approved\": approved}\n",
    "\n",
    "def notification_node(state: State):\n",
    "    # ‚úÖ Good: side effect happens in a separate node\n",
    "    # This runs after approval, so it only executes once\n",
    "    if (state.approved):\n",
    "        send_notification(\n",
    "            user_id=state[\"user_id\"],\n",
    "            status=\"approved\"\n",
    "        )\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1643174c",
   "metadata": {},
   "source": [
    "### Debugging with interrupts\n",
    "\n",
    "To debug and test a graph, you can use static interrupts as breakpoints to step through the graph execution one node at a time. Static interrupts are triggered at defined points either before or after a node executes. You can set these by specifying interrupt_before and interrupt_after when compiling the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbd95ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"some_thread\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Run the graph until the breakpoint\n",
    "graph.invoke(\n",
    "    inputs,\n",
    "    interrupt_before=[\"node_a\"],  \n",
    "    interrupt_after=[\"node_b\", \"node_c\"],  \n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# Resume the graph\n",
    "graph.invoke(None, config=config)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f2e11e",
   "metadata": {},
   "source": [
    "## Memory\n",
    "\n",
    "AI applications need memory to share context across multiple interactions. In LangGraph, you can add two types of memory:\n",
    "1. Add short-term memory as a part of your agent‚Äôs state to enable multi-turn conversations.\n",
    "2. Add long-term memory to store user-specific or application-level data across sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3e3cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use in Production \n",
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    "from langgraph.checkpoint.redis import RedisSaver\n",
    "from langgraph.checkpoint.mongodb import MongoDBSaver  \n",
    "\n",
    "DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\n",
    "with PostgresSaver.from_conn_string(DB_URI) as checkpointer:  \n",
    "    builder = StateGraph(...)\n",
    "    graph = builder.compile(checkpointer=checkpointer)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30d5d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.graph import StateGraph, MessagesState, START\n",
    "from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver  \n",
    "\n",
    "model = init_chat_model(model=\"claude-haiku-4-5-20251001\")\n",
    "\n",
    "DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\n",
    "async with AsyncPostgresSaver.from_conn_string(DB_URI) as checkpointer:  \n",
    "    # await checkpointer.setup()\n",
    "\n",
    "    async def call_model(state: MessagesState):\n",
    "        response = await model.ainvoke(state[\"messages\"])\n",
    "        return {\"messages\": response}\n",
    "\n",
    "    builder = StateGraph(MessagesState)\n",
    "    builder.add_node(call_model)\n",
    "    builder.add_edge(START, \"call_model\")\n",
    "\n",
    "    graph = builder.compile(checkpointer=checkpointer)  \n",
    "\n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"thread_id\": \"1\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    async for chunk in graph.astream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n",
    "        config,  \n",
    "        stream_mode=\"values\"\n",
    "    ):\n",
    "        chunk[\"messages\"][-1].pretty_print()\n",
    "\n",
    "    async for chunk in graph.astream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n",
    "        config,  \n",
    "        stream_mode=\"values\"\n",
    "    ):\n",
    "        chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eec9c75",
   "metadata": {},
   "source": [
    "### Prebuilt memory tools\n",
    "#### LangMem\n",
    "LangMem is a LangChain-maintained library that offers tools for managing long-term memories in your agent.\n",
    "LangMem helps agents learn and adapt from their interactions over time.\n",
    "It provides tooling to extract important information from conversations, optimize agent behavior through prompt refinement, and maintain long-term memory.\n",
    "It offers both functional primitives you can use with any storage system and native integration with LangGraph's storage layer.\n",
    "This lets your agents continuously improve, personalize their responses, and maintain consistent behavior across sessions.\n",
    "\n",
    "Key features¬∂\n",
    "1. üß© Core memory API that works with any storage system\n",
    "2. üß† Memory management tools that agents can use to record and search information during active conversations \"in the hot path\"\n",
    "3. ‚öôÔ∏è Background memory manager that automatically extracts, consolidates, and updates agent knowledge\n",
    "4. ‚ö° Native integration with LangGraph's Long-term Memory Store, available by default in all LangGraph Platform deployments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92724c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core components \n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langmem import create_manage_memory_tool, create_search_memory_tool\n",
    "\n",
    "# Set up storage \n",
    "store = InMemoryStore(\n",
    "    index={\n",
    "        \"dims\": 1536,\n",
    "        \"embed\": \"openai:text-embedding-3-small\",\n",
    "    }\n",
    ") \n",
    "\n",
    "# Create an agent with memory capabilities \n",
    "agent = create_react_agent(\n",
    "    \"anthropic:claude-3-5-sonnet-latest\",\n",
    "    tools=[\n",
    "        # Memory tools use LangGraph's BaseStore for persistence (4)\n",
    "        create_manage_memory_tool(namespace=(\"memories\",)),\n",
    "        create_search_memory_tool(namespace=(\"memories\",)),\n",
    "    ],\n",
    "    store=store,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34923a39",
   "metadata": {},
   "source": [
    "## Database management of long-term memory\n",
    "\n",
    "If you are using any database-backed persistence implementation (such as Postgres or Redis) to store short and/or long-term memory, you will need to run migrations to set up the required schema before you can use it with your database.\n",
    "\n",
    "By convention, most database-specific libraries define a setup() method on the checkpointer or store instance that runs the required migrations. However, you should check with your specific implementation of BaseCheckpointSaver or BaseStore to confirm the exact method name and usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4cdb71",
   "metadata": {},
   "source": [
    "## Subgraphs\n",
    "\n",
    "A subgraph is a graph that is used as a node in another graph.\n",
    "\n",
    "Subgraphs are useful for:\n",
    "1. Building multi-agent systems\n",
    "2. Re-using a set of nodes in multiple graphs\n",
    "3. Distributing development: when you want different teams to work on different parts of the graph independently, you can define each part as a subgraph, and as long as the subgraph interface (the input and output schemas) is respected, the parent graph can be built without knowing any details of the subgraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b11b0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke a graph from a node\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.state import StateGraph, START\n",
    "\n",
    "class SubgraphState(TypedDict):\n",
    "    bar: str\n",
    "\n",
    "# Subgraph\n",
    "\n",
    "def subgraph_node_1(state: SubgraphState):\n",
    "    return {\"bar\": \"hi! \" + state[\"bar\"]}\n",
    "\n",
    "subgraph_builder = StateGraph(SubgraphState)\n",
    "subgraph_builder.add_node(subgraph_node_1)\n",
    "subgraph_builder.add_edge(START, \"subgraph_node_1\")\n",
    "subgraph = subgraph_builder.compile()\n",
    "\n",
    "# Parent graph\n",
    "\n",
    "class State(TypedDict):\n",
    "    foo: str\n",
    "\n",
    "def call_subgraph(state: State):\n",
    "    # Transform the state to the subgraph state\n",
    "    subgraph_output = subgraph.invoke({\"bar\": state[\"foo\"]})  \n",
    "    # Transform response back to the parent state\n",
    "    return {\"foo\": subgraph_output[\"bar\"]}\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"node_1\", call_subgraph)\n",
    "builder.add_edge(START, \"node_1\")\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2f67f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a graph as a node\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.state import StateGraph, START\n",
    "\n",
    "# Define subgraph\n",
    "class SubgraphState(TypedDict):\n",
    "    foo: str  # shared with parent graph state\n",
    "    bar: str  # private to SubgraphState\n",
    "\n",
    "def subgraph_node_1(state: SubgraphState):\n",
    "    return {\"bar\": \"bar\"}\n",
    "\n",
    "def subgraph_node_2(state: SubgraphState):\n",
    "    # note that this node is using a state key ('bar') that is only available in the subgraph\n",
    "    # and is sending update on the shared state key ('foo')\n",
    "    return {\"foo\": state[\"foo\"] + state[\"bar\"]}\n",
    "\n",
    "subgraph_builder = StateGraph(SubgraphState)\n",
    "subgraph_builder.add_node(subgraph_node_1)\n",
    "subgraph_builder.add_node(subgraph_node_2)\n",
    "subgraph_builder.add_edge(START, \"subgraph_node_1\")\n",
    "subgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\n",
    "subgraph = subgraph_builder.compile()\n",
    "\n",
    "# Define parent graph\n",
    "class ParentState(TypedDict):\n",
    "    foo: str\n",
    "\n",
    "def node_1(state: ParentState):\n",
    "    return {\"foo\": \"hi! \" + state[\"foo\"]}\n",
    "\n",
    "builder = StateGraph(ParentState)\n",
    "builder.add_node(\"node_1\", node_1)\n",
    "builder.add_node(\"node_2\", subgraph)\n",
    "builder.add_edge(START, \"node_1\")\n",
    "builder.add_edge(\"node_1\", \"node_2\")\n",
    "graph = builder.compile()\n",
    "\n",
    "for chunk in graph.stream({\"foo\": \"foo\"}):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb36d76b",
   "metadata": {},
   "source": [
    "If you want the subgraph to have its own memory, you can compile it with the appropriate checkpointer option. This is useful in multi-agent systems, if you want agents to keep track of their internal message histories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb7182e",
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph_builder = StateGraph(...)\n",
    "subgraph = subgraph_builder.compile(checkpointer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bf42d7",
   "metadata": {},
   "source": [
    "# View subgraph state\n",
    "\n",
    "When you enable persistence, you can inspect the graph state (checkpoint) via the appropriate method. To view the subgraph state, you can use the subgraphs option.\n",
    "You can inspect the graph state via graph.get_state(config). To view the subgraph state, you can use graph.get_state(config, subgraphs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1cbe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.types import interrupt, Command\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class State(TypedDict):\n",
    "    foo: str\n",
    "\n",
    "# Subgraph\n",
    "\n",
    "def subgraph_node_1(state: State):\n",
    "    value = interrupt(\"Provide value:\")\n",
    "    return {\"foo\": state[\"foo\"] + value}\n",
    "\n",
    "subgraph_builder = StateGraph(State)\n",
    "subgraph_builder.add_node(subgraph_node_1)\n",
    "subgraph_builder.add_edge(START, \"subgraph_node_1\")\n",
    "\n",
    "subgraph = subgraph_builder.compile()\n",
    "\n",
    "# Parent graph\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"node_1\", subgraph)\n",
    "builder.add_edge(START, \"node_1\")\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "graph.invoke({\"foo\": \"\"}, config)\n",
    "parent_state = graph.get_state(config)\n",
    "\n",
    "# This will be available only when the subgraph is interrupted.\n",
    "# Once you resume the graph, you won't be able to access the subgraph state.\n",
    "subgraph_state = graph.get_state(config, subgraphs=True).tasks[0].state\n",
    "\n",
    "# resume the subgraph\n",
    "graph.invoke(Command(resume=\"bar\"), config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428fb5e1",
   "metadata": {},
   "source": [
    "## Stream subgraph outputs\n",
    "\n",
    "To include outputs from subgraphs in the streamed outputs, you can set the subgraphs option in the stream method of the parent graph. This will stream outputs from both the parent graph and any subgraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fa730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.state import StateGraph, START\n",
    "\n",
    "# Define subgraph\n",
    "class SubgraphState(TypedDict):\n",
    "    foo: str\n",
    "    bar: str\n",
    "\n",
    "def subgraph_node_1(state: SubgraphState):\n",
    "    return {\"bar\": \"bar\"}\n",
    "\n",
    "def subgraph_node_2(state: SubgraphState):\n",
    "    # note that this node is using a state key ('bar') that is only available in the subgraph\n",
    "    # and is sending update on the shared state key ('foo')\n",
    "    return {\"foo\": state[\"foo\"] + state[\"bar\"]}\n",
    "\n",
    "subgraph_builder = StateGraph(SubgraphState)\n",
    "subgraph_builder.add_node(subgraph_node_1)\n",
    "subgraph_builder.add_node(subgraph_node_2)\n",
    "subgraph_builder.add_edge(START, \"subgraph_node_1\")\n",
    "subgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\n",
    "subgraph = subgraph_builder.compile()\n",
    "\n",
    "# Define parent graph\n",
    "class ParentState(TypedDict):\n",
    "    foo: str\n",
    "\n",
    "def node_1(state: ParentState):\n",
    "    return {\"foo\": \"hi! \" + state[\"foo\"]}\n",
    "\n",
    "builder = StateGraph(ParentState)\n",
    "builder.add_node(\"node_1\", node_1)\n",
    "builder.add_node(\"node_2\", subgraph)\n",
    "builder.add_edge(START, \"node_1\")\n",
    "builder.add_edge(\"node_1\", \"node_2\")\n",
    "graph = builder.compile()\n",
    "\n",
    "for chunk in graph.stream(\n",
    "    {\"foo\": \"foo\"},\n",
    "    stream_mode=\"updates\",\n",
    "    subgraphs=True, \n",
    "):\n",
    "    print(chunk)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
