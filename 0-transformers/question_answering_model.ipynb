{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering\n",
    "\n",
    "### Using Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e250367b0072413dbcc422cac84f013e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "503d8c1697e14cc49ce2ad67b63feab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da573445822140908aae1fc227a9c7c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ad48a4d45814e3290c866b3f2d935a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1424dc6f5efc4cff88cc347756b2cd6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ppl = pipeline(task = \"question-answering\", model=\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "question =\"What is Jersey Act?\"\n",
    "context = \"\"\"The Jersey Act was a rule introduced in 1913 by the British Jockey Club which effectively barred horses not bred in the United Kingdom, Ireland, or certain Commonwealth countries from being registered in the General Stud Book. The act was primarily aimed at American-bred Thoroughbreds, which were often descended from horses with unknown or non-traditional bloodlines due to the widespread use of non-Thoroughbred mares in the United States. The Jersey Act was eventually repealed in 1949, allowing American-bred horses to be registered in the General Stud Book once again.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.1255001574754715, 'start': 19, 'end': 25, 'answer': 'a rule'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = ppl(question=question, context=context)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "ckpt = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(ckpt)\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tokenizer(question, context, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2054,  2003,  3933,  2552,  1029,   102,  1996,  3933,  2552,\n",
       "          2001,  1037,  3627,  3107,  1999,  5124,  2011,  1996,  2329, 13989,\n",
       "          2252,  2029,  6464, 15605,  5194,  2025, 13680,  1999,  1996,  2142,\n",
       "          2983,  1010,  3163,  1010,  2030,  3056,  5663,  3032,  2013,  2108,\n",
       "          5068,  1999,  1996,  2236, 16054,  2338,  1012,  1996,  2552,  2001,\n",
       "          3952,  6461,  2012,  2137,  1011, 13680, 18359,  2015,  1010,  2029,\n",
       "          2020,  2411,  9287,  2013,  5194,  2007,  4242,  2030,  2512,  1011,\n",
       "          3151,  2668, 12735,  2349,  2000,  1996,  6923,  2224,  1997,  2512,\n",
       "          1011, 18359, 11941,  2015,  1999,  1996,  2142,  2163,  1012,  1996,\n",
       "          3933,  2552,  2001,  2776, 21492,  1999,  4085,  1010,  4352,  2137,\n",
       "          1011, 13680,  5194,  2000,  2022,  5068,  1999,  1996,  2236, 16054,\n",
       "          2338,  2320,  2153,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2054,  2003,  3933,  2552,  1029,   102,  1996,  3933,  2552,\n",
       "          2001,  1037,  3627,  3107,  1999,  5124,  2011,  1996,  2329, 13989,\n",
       "          2252,  2029,  6464, 15605,  5194,  2025, 13680,  1999,  1996,  2142,\n",
       "          2983,  1010,  3163,  1010,  2030,  3056,  5663,  3032,  2013,  2108,\n",
       "          5068,  1999,  1996,  2236, 16054,  2338,  1012,  1996,  2552,  2001,\n",
       "          3952,  6461,  2012,  2137,  1011, 13680, 18359,  2015,  1010,  2029,\n",
       "          2020,  2411,  9287,  2013,  5194,  2007,  4242,  2030,  2512,  1011,\n",
       "          3151,  2668, 12735,  2349,  2000,  1996,  6923,  2224,  1997,  2512,\n",
       "          1011, 18359, 11941,  2015,  1999,  1996,  2142,  2163,  1012,  1996,\n",
       "          3933,  2552,  2001,  2776, 21492,  1999,  4085,  1010,  4352,  2137,\n",
       "          1011, 13680,  5194,  2000,  2022,  5068,  1999,  1996,  2236, 16054,\n",
       "          2338,  2320,  2153,  1012,   102]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'what',\n",
       " 'is',\n",
       " 'jersey',\n",
       " 'act',\n",
       " '?',\n",
       " '[SEP]',\n",
       " 'the',\n",
       " 'jersey',\n",
       " 'act',\n",
       " 'was',\n",
       " 'a',\n",
       " 'rule',\n",
       " 'introduced',\n",
       " 'in',\n",
       " '1913',\n",
       " 'by',\n",
       " 'the',\n",
       " 'british',\n",
       " 'jockey',\n",
       " 'club',\n",
       " 'which',\n",
       " 'effectively',\n",
       " 'barred',\n",
       " 'horses',\n",
       " 'not',\n",
       " 'bred',\n",
       " 'in',\n",
       " 'the',\n",
       " 'united',\n",
       " 'kingdom',\n",
       " ',',\n",
       " 'ireland',\n",
       " ',',\n",
       " 'or',\n",
       " 'certain',\n",
       " 'commonwealth',\n",
       " 'countries',\n",
       " 'from',\n",
       " 'being',\n",
       " 'registered',\n",
       " 'in',\n",
       " 'the',\n",
       " 'general',\n",
       " 'stud',\n",
       " 'book',\n",
       " '.',\n",
       " 'the',\n",
       " 'act',\n",
       " 'was',\n",
       " 'primarily',\n",
       " 'aimed',\n",
       " 'at',\n",
       " 'american',\n",
       " '-',\n",
       " 'bred',\n",
       " 'thoroughbred',\n",
       " '##s',\n",
       " ',',\n",
       " 'which',\n",
       " 'were',\n",
       " 'often',\n",
       " 'descended',\n",
       " 'from',\n",
       " 'horses',\n",
       " 'with',\n",
       " 'unknown',\n",
       " 'or',\n",
       " 'non',\n",
       " '-',\n",
       " 'traditional',\n",
       " 'blood',\n",
       " '##lines',\n",
       " 'due',\n",
       " 'to',\n",
       " 'the',\n",
       " 'widespread',\n",
       " 'use',\n",
       " 'of',\n",
       " 'non',\n",
       " '-',\n",
       " 'thoroughbred',\n",
       " 'mare',\n",
       " '##s',\n",
       " 'in',\n",
       " 'the',\n",
       " 'united',\n",
       " 'states',\n",
       " '.',\n",
       " 'the',\n",
       " 'jersey',\n",
       " 'act',\n",
       " 'was',\n",
       " 'eventually',\n",
       " 'repealed',\n",
       " 'in',\n",
       " '1949',\n",
       " ',',\n",
       " 'allowing',\n",
       " 'american',\n",
       " '-',\n",
       " 'bred',\n",
       " 'horses',\n",
       " 'to',\n",
       " 'be',\n",
       " 'registered',\n",
       " 'in',\n",
       " 'the',\n",
       " 'general',\n",
       " 'stud',\n",
       " 'book',\n",
       " 'once',\n",
       " 'again',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert Ids to tokens\n",
    "tokenizer.convert_ids_to_tokens(input[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    output = model(**input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-6.1737, -5.4501, -7.6491, -7.8417, -8.9177, -9.6428, -6.1737,  2.1326,\n",
       "         -0.6377, -3.9003,  1.5278,  5.8120,  2.4855, -2.3204, -5.0819, -2.0535,\n",
       "         -5.1888, -3.6642, -1.1930, -4.1715, -4.7875, -0.3869,  2.3690,  1.3900,\n",
       "         -2.2416, -3.7630, -4.7978, -5.9275, -6.0129, -4.7044, -5.1895, -7.8884,\n",
       "         -5.1640, -8.0797, -7.7593, -5.3721, -4.1696, -4.3681, -3.6616, -4.2972,\n",
       "         -2.7641, -5.3319, -4.0441, -3.2806, -3.8089, -4.3647, -6.1738, -0.3728,\n",
       "         -3.0377, -5.9554, -3.4978, -3.0977, -5.9246, -4.0664, -6.7045, -6.1841,\n",
       "         -4.6966, -6.3143, -7.6931, -6.9967, -6.5963, -5.4772, -4.2514, -6.7579,\n",
       "         -4.2497, -7.3626, -5.1899, -8.1484, -4.7469, -7.9955, -7.3069, -5.9191,\n",
       "         -6.3765, -6.6210, -8.0237, -7.1373, -5.2717, -6.4789, -7.9595, -5.3296,\n",
       "         -8.5280, -5.8391, -6.7350, -7.8584, -7.9116, -7.7512, -6.5337, -6.6001,\n",
       "         -6.4466,  0.2188, -2.3120, -5.3184, -4.9042, -4.9141, -3.2463, -5.9475,\n",
       "         -4.6554, -6.8602, -2.2605, -4.7031, -8.0941, -6.3039, -5.3423, -6.4424,\n",
       "         -6.7756, -4.3906, -7.0049, -6.2850, -5.0761, -5.4311, -6.0084, -7.2921,\n",
       "         -7.1153, -7.3950, -6.1737]]), end_logits=tensor([[-1.3585e+00, -4.6650e+00, -6.5969e+00, -7.7792e+00, -6.1657e+00,\n",
       "         -7.7315e+00, -1.3584e+00, -3.4464e+00, -2.9205e+00, -7.0783e-01,\n",
       "         -4.4563e+00, -2.5681e+00,  3.0960e+00, -7.6461e-01, -5.8951e+00,\n",
       "          9.0966e-01, -4.9852e+00, -6.3683e+00, -3.5901e+00, -3.8270e+00,\n",
       "          2.1364e+00, -5.0235e+00, -4.4133e+00, -2.8323e+00, -1.9584e+00,\n",
       "         -3.8427e+00, -4.0378e-01, -4.7187e+00, -5.6399e+00, -4.4165e+00,\n",
       "          7.8568e-01, -4.5461e+00, -7.0119e-02, -3.0734e+00, -5.2983e+00,\n",
       "         -4.6767e+00, -2.2859e+00,  2.5275e+00, -4.1970e+00, -4.7119e+00,\n",
       "         -1.6004e-01, -4.4311e+00, -5.2046e+00, -4.0693e+00, -3.0540e+00,\n",
       "          4.5564e+00, -1.3588e+00, -6.0503e+00, -3.7560e+00, -7.2332e+00,\n",
       "         -6.7829e+00, -5.4183e+00, -6.6079e+00, -5.7818e+00, -6.5017e+00,\n",
       "         -4.6865e+00, -3.7394e+00,  2.0870e-01, -2.1295e+00, -6.2163e+00,\n",
       "         -7.2457e+00, -6.9343e+00, -5.8508e+00, -7.4644e+00, -5.6000e+00,\n",
       "         -7.7974e+00, -6.1119e+00, -7.9579e+00, -7.0215e+00, -7.3041e+00,\n",
       "         -6.4524e+00, -5.7715e+00, -1.0535e+00, -6.0041e+00, -7.6204e+00,\n",
       "         -7.9351e+00, -7.0312e+00, -6.0120e+00, -8.0601e+00, -7.4370e+00,\n",
       "         -7.9661e+00, -5.7112e+00, -6.0977e+00, -3.3146e+00, -7.7467e+00,\n",
       "         -7.7627e+00, -6.6942e+00, -1.6568e+00, -1.2369e+00, -5.8930e+00,\n",
       "         -4.6699e+00, -3.0736e+00, -6.7752e+00, -6.3340e+00, -2.6376e+00,\n",
       "         -6.8777e+00, -1.1714e-01, -2.3007e+00, -6.5843e+00, -6.4530e+00,\n",
       "         -7.1350e+00, -5.6436e+00, -3.5779e+00, -6.8240e+00, -6.7375e+00,\n",
       "         -2.7493e+00, -6.3151e+00, -6.9281e+00, -6.0430e+00, -5.0084e+00,\n",
       "          9.0415e-01, -5.2888e+00, -3.1206e-01,  2.2055e-03, -1.3586e+00]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It will generate two tensors: start_logits and end_logits, which contain the scores for each token being the start or end of the answer span.\n",
    "start_scores = output[\"start_logits\"].argmax()\n",
    "start_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(45)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_scores = output[\"end_logits\"].argmax()\n",
    "end_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a rule introduced in 1913 by the british jockey club which effectively barred horses not bred in the united kingdom, ireland, or certain commonwealth countries from being registered in the general stud book'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = input[\"input_ids\"][0][start_scores : end_scores + 1]\n",
    "answer = tokenizer.decode(input_ids)\n",
    "answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
