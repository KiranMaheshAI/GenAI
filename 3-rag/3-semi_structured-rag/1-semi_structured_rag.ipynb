{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "857516fa",
   "metadata": {},
   "source": [
    "### Semi-structured RAG\n",
    "\n",
    "Many documents contain a mixture of content types, including text and tables.\n",
    "\n",
    "Semi-structured data can be challenging for conventional RAG for at least two reasons:\n",
    "\n",
    "Text splitting may break up tables, corrupting the data in retrieval\n",
    "Embedding tables may pose challenges for semantic similarity search.\n",
    "\n",
    "We will use `Unstructured` to parse both text and tables from documents (PDFs).\n",
    "\n",
    "We will use the `multi-vector retriever` to store raw tables, text along with table summaries better suited for retrieval.\n",
    "\n",
    "We will use `LCEL` to implement the chains used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96ca0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install langchain unstructured pydantic lxml langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0ada62",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install tesseract\n",
    "!pip3 install poppler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13b2c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee098f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Add Homebrew's bin directory to PATH so Python can find poppler utilities\n",
    "os.environ[\"PATH\"] = \"/opt/homebrew/bin:\" + os.environ.get(\"PATH\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad146fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/I572648/Library/CloudStorage/OneDrive-SAPSE/Desktop/Git/GenAI/3-rag/3-semi_structured-rag/data/llava.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbeb9fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "# Get elements\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=path,\n",
    "    # Unstructured first finds embedded image blocks\n",
    "    extract_images_in_pdf=False,\n",
    "    # Use layout model (YOLOX) to get bounding boxes (for tables) and find titles\n",
    "    # Titles are any sub-section of the document\n",
    "    infer_table_structure=True,\n",
    "    # Post processing to aggregate text once we have the title\n",
    "    chunking_strategy=\"by_title\",\n",
    "    # Chunking params to aggregate text blocks\n",
    "    # Attempt to create a new chunk 3800 chars\n",
    "    # Attempt to keep chunks > 2000 chars\n",
    "    max_characters=4000,  # Maximum number of characters per chunk\n",
    "    new_after_n_chars=3800,  # New chunk after this number of characters. Hard limit.\n",
    "    combine_text_under_n_chars=2000,  # Combine text blocks under this number of characters with previous text block\n",
    "    image_output_dir_path='/Users/I572648/Library/CloudStorage/OneDrive-SAPSE/Desktop/Git/GenAI/3-rag/3-semi_structured-rag/data',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e72b431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"<class 'unstructured.documents.elements.CompositeElement'>\": 27}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary to store counts of each type\n",
    "category_counts = {}\n",
    "\n",
    "for element in raw_pdf_elements:\n",
    "    category = str(type(element))\n",
    "    if category in category_counts:\n",
    "        category_counts[category] += 1\n",
    "    else:\n",
    "        category_counts[category] = 1\n",
    "\n",
    "# Unique_categories will have unique elements\n",
    "unique_categories = set (category_counts.keys())\n",
    "category_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "300a4510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "class Element(BaseModel):\n",
    "    type: str\n",
    "    text: Any\n",
    "\n",
    "\n",
    "# Categorize by type\n",
    "categorized_elements = []\n",
    "for element in raw_pdf_elements:\n",
    "    if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "        categorized_elements.append(Element(type=\"table\", text=str(element)))\n",
    "    elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "        categorized_elements.append(Element(type=\"text\", text=str(element)))\n",
    "\n",
    "# Tables\n",
    "table_elements = [e for e in categorized_elements if e.type == \"table\"]\n",
    "print(len(table_elements))\n",
    "\n",
    "# Text\n",
    "text_elements = [e for e in categorized_elements if e.type == \"text\"]\n",
    "print(len(text_elements))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d93c1e6",
   "metadata": {},
   "source": [
    "### Multi Vector Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "926696fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4787f5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['element'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'multi-vector-retriever-summarization', 'lc_hub_commit_hash': 'd822e5e6d60be1e8e19b1e849a99ab65d384972cfd2414e4281b386e287b122d'}, template='You are an assistant tasked with summarizing tables and text. \\\\ \\nGive a concise summary of the table or text. Table or text chunk: {element}')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "multi_vector_prompt1 = hub.pull(\"rlm/multi-vector-retriever-summarization\")\n",
    "multi_vector_prompt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1fcf22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text. \\ \n",
    "Give a concise summary of the table or text. Table or text chunk: {element} \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "# Summary chain\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\", max_retries=3, request_timeout=20)\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e392dfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to tables\n",
    "tables = [i.text for i in table_elements]\n",
    "table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2d3b9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to texts\n",
    "texts = [i.text for i in text_elements]\n",
    "text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cac9b22",
   "metadata": {},
   "source": [
    "### Add to vectorstore\n",
    "\n",
    "Use Multi Vector Retriever with summaries:\n",
    "\n",
    "InMemoryStore stores the raw text, tables\n",
    "\n",
    "vectorstore stores the embedded summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269af227",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49c4de3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts: 27\n",
      "Number of text_summaries: 27\n",
      "Number of tables: 0\n",
      "Number of table_summaries: 0\n"
     ]
    }
   ],
   "source": [
    "# Check if we have any data\n",
    "print(f\"Number of texts: {len(texts)}\")\n",
    "print(f\"Number of text_summaries: {len(text_summaries)}\")\n",
    "print(f\"Number of tables: {len(tables)}\")\n",
    "print(f\"Number of table_summaries: {len(table_summaries)}\")\n",
    "\n",
    "if len(texts) == 0 and len(tables) == 0:\n",
    "    print(\"\\nWARNING: No texts or tables were extracted from the PDF!\")\n",
    "    print(\"This could mean:\")\n",
    "    print(\"1. The PDF is empty or has no extractable content\")\n",
    "    print(\"2. The extraction parameters need adjustment\")\n",
    "    print(\"3. The model download is still in progress\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80720754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 27 text documents\n",
      "No tables to add\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain_classic.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain_classic.storage import InMemoryStore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(collection_name=\"summaries\", embedding_function=OpenAIEmbeddings())\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "# Add texts (only if we have any)\n",
    "if len(texts) > 0 and len(text_summaries) > 0:\n",
    "    doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
    "    summary_texts = [\n",
    "        Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "        for i, s in enumerate(text_summaries)\n",
    "    ]\n",
    "    retriever.vectorstore.add_documents(summary_texts)\n",
    "    retriever.docstore.mset(list(zip(doc_ids, texts)))\n",
    "    print(f\"Added {len(texts)} text documents\")\n",
    "else:\n",
    "    print(\"No texts to add\")\n",
    "\n",
    "# Add tables (only if we have any)\n",
    "if len(tables) > 0 and len(table_summaries) > 0:\n",
    "    table_ids = [str(uuid.uuid4()) for _ in tables]\n",
    "    summary_tables = [\n",
    "        Document(page_content=s, metadata={id_key: table_ids[i]})\n",
    "        for i, s in enumerate(table_summaries)\n",
    "    ]\n",
    "    retriever.vectorstore.add_documents(summary_tables)\n",
    "    retriever.docstore.mset(list(zip(table_ids, tables)))\n",
    "    print(f\"Added {len(tables)} table documents\")\n",
    "else:\n",
    "    print(\"No tables to add\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f845965",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Prompt template\n",
    "template = \"\"\"Answer the question based only on the following context, which can include text and tables:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# LLM\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "\n",
    "# RAG pipeline\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0e2315ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The ablation on LLaVA-Bench (COCO) with different training data is as follows:\\n\\n- Full data: 83.1 for Conversation, 75.3 for Detail description, 96.5 for Complex reasoning, and 85.1 for All.\\n- Detail + Complex: 81.5 for Conversation (-1.6 compared to Full data), 73.3 for Detail description (-2.0), 90.8 for Complex reasoning (-5.7), and 81.9 for All (-3.2).\\n- Conv + 5% Detail + 10% Complex: 81.0 for Conversation (-2.1), 68.4 for Detail description (-7.1), 91.5 for Complex reasoning (-5.0), and 80.5 for All (-4.4).\\n- Conversation: 76.5 for Conversation (-6.6), 59.8 for Detail description (-16.2), 84.9 for Complex reasoning (-12.4), and 73.8 for All (-11.3).\\n- No Instruction Tuning: 22.0 for Conversation (-61.1), 24.0 for Detail description (-51.3), 18.5 for Complex reasoning (-78.0), and 21.5 for All (-63.6).'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Get Ablation on LLaVA-Bench (COCO) with different training data? \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f417f403",
   "metadata": {},
   "source": [
    "'The ablation on LLaVA-Bench (COCO) with different training data is as follows:\\n\\n- Full data: 83.1 for Conversation, 75.3 for Detail description, 96.5 for Complex reasoning, and 85.1 for All.\\n- Detail + Complex: 81.5 for Conversation (-1.6 compared to Full data), 73.3 for Detail description (-2.0), 90.8 for Complex reasoning (-5.7), and 81.9 for All (-3.2).\\n- Conv + 5% Detail + 10% Complex: 81.0 for Conversation (-2.1), 68.4 for Detail description (-7.1), 91.5 for Complex reasoning (-5.0), and 80.5 for All (-4.4).\\n- Conversation: 76.5 for Conversation (-6.6), 59.8 for Detail description (-16.2), 84.9 for Complex reasoning (-12.4), and 73.8 for All (-11.3).\\n- No Instruction Tuning: 22.0 for Conversation (-61.1), 24.0 for Detail description (-51.3), 18.5 for Complex reasoning (-78.0), and 21.5 for All (-63.6).'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
