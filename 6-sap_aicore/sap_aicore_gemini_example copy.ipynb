{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gemini models in  SAP AI Core\n",
        "\n",
        "Using direct API calls with Vertex AI format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Set Credentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì AI Core credentials configured\n"
          ]
        }
      ],
      "source": [
        "# Path to your AI Core service key JSON file\n",
        "service_key_path = \"../aicore-service-key.json\"\n",
        "\n",
        "# Load service key\n",
        "with open(service_key_path, 'r') as f:\n",
        "    service_key = json.load(f)\n",
        "\n",
        "# Set environment variables\n",
        "os.environ['AICORE_AUTH_URL'] = service_key['url']\n",
        "os.environ['AICORE_CLIENT_ID'] = service_key['clientid']\n",
        "os.environ['AICORE_CLIENT_SECRET'] = service_key['clientsecret']\n",
        "os.environ['AICORE_RESOURCE_GROUP'] = 'default'\n",
        "\n",
        "# Parse serviceurls - it might be a string or already a dict\n",
        "serviceurls = service_key['serviceurls']\n",
        "if isinstance(serviceurls, str):\n",
        "    serviceurls = json.loads(serviceurls)\n",
        "os.environ['AICORE_BASE_URL'] = serviceurls['AI_API_URL']\n",
        "base_url = os.environ['AICORE_BASE_URL']\n",
        "\n",
        "print(\"‚úì AI Core credentials configured\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Get Auth Token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Auth token obtained\n"
          ]
        }
      ],
      "source": [
        "from gen_ai_hub.proxy.core.proxy_clients import get_proxy_client\n",
        "\n",
        "# Get auth headers\n",
        "proxy = get_proxy_client('gen-ai-hub')\n",
        "headers = proxy.get_request_header()\n",
        "\n",
        "print('‚úì Auth token obtained')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3b. Check for Image Generation Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All Available Deployments:\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Summary: 0 image generation model(s) found\n",
            "\n",
            "‚ö†Ô∏è  No image generation models deployed in your SAP AI Core\n",
            "\n",
            "Model: gemini-2.5-pro\n",
            "  Deployment ID: d6ae523ed14c6cc3\n",
            "  üìù Text/Analysis model\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Summary: 0 image generation model(s) found\n",
            "\n",
            "‚ö†Ô∏è  No image generation models deployed in your SAP AI Core\n",
            "\n",
            "Model: gemini-2.0-flash\n",
            "  Deployment ID: d83ad32cbda0399d\n",
            "  üìù Text/Analysis model\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Summary: 0 image generation model(s) found\n",
            "\n",
            "‚ö†Ô∏è  No image generation models deployed in your SAP AI Core\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Summary: 0 image generation model(s) found\n",
            "\n",
            "‚ö†Ô∏è  No image generation models deployed in your SAP AI Core\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Summary: 0 image generation model(s) found\n",
            "\n",
            "‚ö†Ô∏è  No image generation models deployed in your SAP AI Core\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Summary: 0 image generation model(s) found\n",
            "\n",
            "‚ö†Ô∏è  No image generation models deployed in your SAP AI Core\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Summary: 0 image generation model(s) found\n",
            "\n",
            "‚ö†Ô∏è  No image generation models deployed in your SAP AI Core\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Summary: 0 image generation model(s) found\n",
            "\n",
            "‚ö†Ô∏è  No image generation models deployed in your SAP AI Core\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Summary: 0 image generation model(s) found\n",
            "\n",
            "‚ö†Ô∏è  No image generation models deployed in your SAP AI Core\n"
          ]
        }
      ],
      "source": [
        "# List all deployments and check for image generation models\n",
        "deployments = proxy.get_deployments()\n",
        "\n",
        "print('All Available Deployments:')\n",
        "print('='*80)\n",
        "\n",
        "image_models = []\n",
        "\n",
        "for d in deployments:\n",
        "    model_name = d.model_name.lower()\n",
        "    if \"gemini\" in model_name:\n",
        "        print(f\"\\nModel: {d.model_name}\")\n",
        "        print(f\"  Deployment ID: {d.deployment_id}\")\n",
        "        \n",
        "        # Check if it's an image generation model\n",
        "        if any(x in model_name for x in ['dall-e', 'dalle', 'stable-diffusion', 'sd-', 'imagen']):\n",
        "            image_models.append(d)\n",
        "            print(\"  ‚úì IMAGE GENERATION MODEL\")\n",
        "        else:\n",
        "            \n",
        "            print(\"  üìù Text/Analysis model\")\n",
        "\n",
        "    print('\\n' + '='*80)\n",
        "    print(f'\\nSummary: {len(image_models)} image generation model(s) found')\n",
        "\n",
        "    if image_models:\n",
        "        print('\\n‚úì Image generation models:')\n",
        "        for m in image_models:\n",
        "            print(f'  - {m.model_name} (ID: {m.deployment_id})')\n",
        "    else:\n",
        "        print('\\n‚ö†Ô∏è  No image generation models deployed in your SAP AI Core')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Call Gemini 2.5 Pro (Vertex AI Format)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Success!\n",
            "{'candidates': [{'avgLogprobs': -0.6888435347908004, 'content': {'parts': [{'text': 'Of course!\\n\\nAs a text-based AI, I can\\'t generate a visual image file directly. However, I can do the next best thing! I can either:\\n\\n1.  **Describe a cat image in detail** for you to use with an AI image generator (like Midjourney, DALL-E, or Stable Diffusion).\\n2.  **Create a cat for you using text characters** (ASCII art).\\n\\nWhich would you prefer? Here are both options for you!\\n\\n---\\n\\n### 1. Detailed Descriptions for an AI Image Generator\\n\\nCopy and paste one of these prompts into an AI image tool to get a fantastic picture.\\n\\n**Prompt for a Photorealistic Cat:**\\n> A highly detailed, photorealistic close-up of a fluffy Siberian cat sleeping in a patch of sunlight on a wooden floor. The cat\\'s long, silver-tabby fur is soft and distinct. Dust motes dance in the warm, golden sunbeam. The focus is sharp on the cat\\'s face, showing its peaceful expression and twitching whiskers. The background is softly blurred.\\n\\n**Prompt for a Whimsical, Fantasy Cat:**\\n> Digital painting of an ethereal, mystical cat made of swirling cosmic stardust. Its fur is a deep indigo and black, filled with tiny, glowing stars and nebulae. Its eyes are bright, luminous crescent moons. It sits gracefully on a branch under a giant, magical tree with glowing leaves. The style is enchanting and dreamlike.\\n\\n**Prompt for a Cute, Cartoon Cat:**\\n> A simple, adorable cartoon illustration of a small, round, ginger kitten with huge, expressive green eyes. It\\'s playfully batting at a ball of bright blue yarn on a clean, white background. The style is clean, with bold outlines and soft, cheerful colors.\\n\\n---\\n\\n### 2. Cat Image Made from Text (ASCII Art)\\n\\nHere is a classic cat you can copy and paste anywhere!\\n\\n```\\n      /\\\\_/\\\\\\n     ( o.o )\\n      > ^ <\\n```\\n\\nAnd a more detailed one sitting:\\n\\n```\\n    _._     _,-\\'\"\"`-._\\n   (,-.`._,\\'(       |\\\\`-/|\\n       `-.-\\' \\\\ )-`( , o o)\\n             `-    \\\\`_`\"\\'-\\n```'}], 'role': 'model'}, 'finishReason': 'STOP'}], 'createTime': '2025-11-20T06:54:39.664700Z', 'modelVersion': 'gemini-2.5-pro', 'responseId': 'L7seafzIKM6RhMIP652IqAw', 'usageMetadata': {'candidatesTokenCount': 481, 'candidatesTokensDetails': [{'modality': 'TEXT', 'tokenCount': 481}], 'promptTokenCount': 6, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 6}], 'thoughtsTokenCount': 1434, 'totalTokenCount': 1921, 'trafficType': 'ON_DEMAND'}}\n",
            "\n",
            "Response:\n",
            "Of course!\n",
            "\n",
            "As a text-based AI, I can't generate a visual image file directly. However, I can do the next best thing! I can either:\n",
            "\n",
            "1.  **Describe a cat image in detail** for you to use with an AI image generator (like Midjourney, DALL-E, or Stable Diffusion).\n",
            "2.  **Create a cat for you using text characters** (ASCII art).\n",
            "\n",
            "Which would you prefer? Here are both options for you!\n",
            "\n",
            "---\n",
            "\n",
            "### 1. Detailed Descriptions for an AI Image Generator\n",
            "\n",
            "Copy and paste one of these prompts into an AI image tool to get a fantastic picture.\n",
            "\n",
            "**Prompt for a Photorealistic Cat:**\n",
            "> A highly detailed, photorealistic close-up of a fluffy Siberian cat sleeping in a patch of sunlight on a wooden floor. The cat's long, silver-tabby fur is soft and distinct. Dust motes dance in the warm, golden sunbeam. The focus is sharp on the cat's face, showing its peaceful expression and twitching whiskers. The background is softly blurred.\n",
            "\n",
            "**Prompt for a Whimsical, Fantasy Cat:**\n",
            "> Digital painting of an ethereal, mystical cat made of swirling cosmic stardust. Its fur is a deep indigo and black, filled with tiny, glowing stars and nebulae. Its eyes are bright, luminous crescent moons. It sits gracefully on a branch under a giant, magical tree with glowing leaves. The style is enchanting and dreamlike.\n",
            "\n",
            "**Prompt for a Cute, Cartoon Cat:**\n",
            "> A simple, adorable cartoon illustration of a small, round, ginger kitten with huge, expressive green eyes. It's playfully batting at a ball of bright blue yarn on a clean, white background. The style is clean, with bold outlines and soft, cheerful colors.\n",
            "\n",
            "---\n",
            "\n",
            "### 2. Cat Image Made from Text (ASCII Art)\n",
            "\n",
            "Here is a classic cat you can copy and paste anywhere!\n",
            "\n",
            "```\n",
            "      /\\_/\\\n",
            "     ( o.o )\n",
            "      > ^ <\n",
            "```\n",
            "\n",
            "And a more detailed one sitting:\n",
            "\n",
            "```\n",
            "    _._     _,-'\"\"`-._\n",
            "   (,-.`._,'(       |\\`-/|\n",
            "       `-.-' \\ )-`( , o o)\n",
            "             `-    \\`_`\"'-\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "# Gemini 2.5 Pro deployment\n",
        "DEPLOYMENT_ID = 'd6ae523ed14c6cc3'\n",
        "url = f'{base_url}/v2/inference/deployments/{DEPLOYMENT_ID}/models/gemini-2.5-pro:generateContent'\n",
        "\n",
        "# Vertex AI format payload\n",
        "payload = {\n",
        "    'contents': [\n",
        "        {\n",
        "            'role': 'user',\n",
        "            'parts': [{'text': 'can you generate the cat image'}]\n",
        "        }\n",
        "    ],\n",
        "    'generationConfig': {\n",
        "        'temperature': 0.7,\n",
        "        'maxOutputTokens': 10000\n",
        "    }\n",
        "}\n",
        "\n",
        "response = requests.post(url, headers=headers, json=payload)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    result = response.json()\n",
        "    print('‚úì Success!')\n",
        "    print(result)\n",
        "    print(f\"\\nResponse:\\n{result['candidates'][0]['content']['parts'][0]['text']}\")\n",
        "else:\n",
        "    print(f'Error {response.status_code}: {response.text}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4b. Test Image Generation with Gemini 2.0 Flash\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Success!\n",
            "\n",
            "Response:\n",
            "I am unable to generate images directly. I am a text-based AI.\n",
            "\n",
            "However, I can give you some ideas of what to search for online, or suggest prompts for an image generation tool if you have access to one. For example:\n",
            "\n",
            "**Ideas for Image Searches:**\n",
            "\n",
            "*   \"Cute kitten\"\n",
            "*   \"Sleeping cat\"\n",
            "*   \"Funny cat meme\"\n",
            "*   \"Cat portrait\"\n",
            "*   \"Cartoon cat\"\n",
            "\n",
            "**Prompts for Image Generation Tools:**\n",
            "\n",
            "*   \"A fluffy ginger cat sitting in a sunbeam\"\n",
            "*   \"A sleek black cat with green eyes staring intensely\"\n",
            "*   \"A whimsical illustration of a cat wearing a tiny hat\"\n",
            "*   \"A photorealistic image of a tabby cat curled up on a couch\"\n",
            "*   \"A pixel art cat sprite\"\n",
            "\n",
            "I hope this helps you find the purr-fect cat image! üê±\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Note: Gemini models (both 2.5 Pro and 2.0 Flash) can ANALYZE images\n",
            "but cannot GENERATE images. They are text and image understanding models.\n",
            "For image generation, you would need models like DALL-E or Stable Diffusion.\n"
          ]
        }
      ],
      "source": [
        "# Test if Gemini 2.0 Flash can generate images\n",
        "DEPLOYMENT_ID_2_0 = 'd83ad32cbda0399d'\n",
        "url = f'{base_url}/v2/inference/deployments/{DEPLOYMENT_ID_2_0}/models/gemini-2.0-flash:generateContent'\n",
        "\n",
        "# Try to ask it to generate an image\n",
        "payload = {\n",
        "    'contents': [\n",
        "        {\n",
        "            'role': 'user',\n",
        "            'parts': [{'text': 'can you generate a cat image'}]\n",
        "        }\n",
        "    ],\n",
        "    'generationConfig': {\n",
        "        'temperature': 0.7,\n",
        "        'maxOutputTokens': 1000\n",
        "    }\n",
        "}\n",
        "\n",
        "response = requests.post(url, headers=headers, json=payload)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    result = response.json()\n",
        "    print('‚úì Success!')\n",
        "    print(f\"\\nResponse:\\n{result['candidates'][0]['content']['parts'][0]['text']}\")\n",
        "else:\n",
        "    print(f'Error {response.status_code}: {response.text}')\n",
        "    \n",
        "print('\\n' + '='*80)\n",
        "print('Note: Gemini models (both 2.5 Pro and 2.0 Flash) can ANALYZE images')\n",
        "print('but cannot GENERATE images. They are text and image understanding models.')\n",
        "print('For image generation, you would need models like DALL-E or Stable Diffusion.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Helper Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing Gemini 2.5 Pro:\n",
            "Hello there! üëã\n",
            "\n",
            "How can I help you today?\n",
            "\n",
            "Testing Gemini 2.0 Flash:\n",
            "Hello! How can I help you today?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def ask_gemini(prompt, temperature=0.7, max_tokens=500, model='2.5-pro'):\n",
        "    \"\"\"Call Gemini via Vertex AI format\"\"\"\n",
        "    # Choose deployment\n",
        "    deployments = {\n",
        "        '2.5-pro': 'd6ae523ed14c6cc3',\n",
        "        '2.0-flash': 'd83ad32cbda0399d'\n",
        "    }\n",
        "    deployment_id = deployments.get(model, deployments['2.5-pro'])\n",
        "    model_name = 'gemini-2.5-pro' if model == '2.5-pro' else 'gemini-2.0-flash'\n",
        "    \n",
        "    url = f'{AI_API_URL}/v2/inference/deployments/{deployment_id}/models/{model_name}:generateContent'\n",
        "    \n",
        "    payload = {\n",
        "        'contents': [{\n",
        "            'role': 'user',\n",
        "            'parts': [{'text': prompt}]\n",
        "        }],\n",
        "        'generationConfig': {\n",
        "            'temperature': temperature,\n",
        "            'maxOutputTokens': max_tokens\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    resp = requests.post(url, headers=headers, json=payload)\n",
        "    \n",
        "    if resp.status_code == 200:\n",
        "        return resp.json()['candidates'][0]['content']['parts'][0]['text']\n",
        "    raise Exception(f'Error {resp.status_code}: {resp.text}')\n",
        "\n",
        "# Test with 2.5 Pro\n",
        "print('Testing Gemini 2.5 Pro:')\n",
        "print(ask_gemini('Say hello!', model='2.5-pro'))\n",
        "\n",
        "print('\\nTesting Gemini 2.0 Flash:')\n",
        "print(ask_gemini('Say hello!', model='2.0-flash'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gemini 2.0 Flash - All Available Endpoints\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. :generateContent (Standard - Synchronous)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Endpoint 1: Standard generateContent\n",
        "DEPLOYMENT_ID_2_0 = 'd83ad32cbda0399d'\n",
        "url = f'{AI_API_URL}/v2/inference/deployments/{DEPLOYMENT_ID_2_0}/models/gemini-2.0-flash:generateContent'\n",
        "\n",
        "payload = {\n",
        "    'contents': [{\n",
        "        'role': 'user',\n",
        "        'parts': [{'text': 'Explain quantum computing in one sentence.'}]\n",
        "    }],\n",
        "    'generationConfig': {\n",
        "        'temperature': 0.7,\n",
        "        'maxOutputTokens': 100\n",
        "    }\n",
        "}\n",
        "\n",
        "response = requests.post(url, headers=headers, json=payload)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    result = response.json()\n",
        "    print('‚úì Standard Generation Success!')\n",
        "    print(f\"\\nResponse:\\n{result['candidates'][0]['content']['parts'][0]['text']}\")\n",
        "else:\n",
        "    print(f'Error {response.status_code}: {response.text}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. :streamGenerateContent (Streaming - Real-time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Endpoint 2: Streaming generateContent\n",
        "url = f'{AI_API_URL}/v2/inference/deployments/{DEPLOYMENT_ID_2_0}/models/gemini-2.0-flash:streamGenerateContent'\n",
        "\n",
        "payload = {\n",
        "    'contents': [{\n",
        "        'role': 'user',\n",
        "        'parts': [{'text': 'Write a short poem about AI.'}]\n",
        "    }],\n",
        "    'generationConfig': {\n",
        "        'temperature': 0.9,\n",
        "        'maxOutputTokens': 200\n",
        "    }\n",
        "}\n",
        "\n",
        "print('‚úì Streaming Response:')\n",
        "print('-' * 60)\n",
        "\n",
        "response = requests.post(url, headers=headers, json=payload, stream=True)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    # Process streaming response\n",
        "    import json\n",
        "    for line in response.iter_lines():\n",
        "        if line:\n",
        "            try:\n",
        "                # Parse each JSON chunk\n",
        "                chunk = json.loads(line.decode('utf-8'))\n",
        "                if 'candidates' in chunk and len(chunk['candidates']) > 0:\n",
        "                    text = chunk['candidates'][0]['content']['parts'][0]['text']\n",
        "                    print(text, end='', flush=True)\n",
        "            except json.JSONDecodeError:\n",
        "                continue\n",
        "    print('\\n' + '-' * 60)\n",
        "    print('‚úì Streaming complete!')\n",
        "else:\n",
        "    print(f'Error {response.status_code}: {response.text}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. :countTokens (Token Counting)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Endpoint 3: Count tokens in a prompt\n",
        "url = f'{AI_API_URL}/v2/inference/deployments/{DEPLOYMENT_ID_2_0}/models/gemini-2.0-flash:countTokens'\n",
        "\n",
        "test_prompt = \"\"\"\n",
        "Write a comprehensive analysis of the impact of artificial intelligence \n",
        "on modern business operations, including specific examples from finance, \n",
        "healthcare, and manufacturing sectors.\n",
        "\"\"\"\n",
        "\n",
        "payload = {\n",
        "    'contents': [{\n",
        "        'role': 'user',\n",
        "        'parts': [{'text': test_prompt}]\n",
        "    }]\n",
        "}\n",
        "\n",
        "response = requests.post(url, headers=headers, json=payload)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    result = response.json()\n",
        "    print('‚úì Token Count Success!')\n",
        "    print(f\"\\nPrompt: {test_prompt[:100]}...\")\n",
        "    print(f\"\\nTotal Tokens: {result.get('totalTokens', 'N/A')}\")\n",
        "    \n",
        "    # Some implementations return more details\n",
        "    if 'tokensPerCandidate' in result:\n",
        "        print(f\"Tokens per candidate: {result['tokensPerCandidate']}\")\n",
        "else:\n",
        "    print(f'Error {response.status_code}: {response.text}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. :embedContent (Embeddings - for semantic search)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Endpoint 4: Generate embeddings (if supported)\n",
        "# Note: Not all Gemini models support embeddings. This may require a specific embedding model.\n",
        "url = f'{AI_API_URL}/v2/inference/deployments/{DEPLOYMENT_ID_2_0}/models/gemini-2.0-flash:embedContent'\n",
        "\n",
        "texts_to_embed = [\n",
        "    \"Artificial intelligence is transforming business\",\n",
        "    \"Machine learning powers modern applications\",\n",
        "    \"SAP AI Core enables AI deployment\"\n",
        "]\n",
        "\n",
        "payload = {\n",
        "    'content': {\n",
        "        'parts': [{'text': texts_to_embed[0]}]\n",
        "    }\n",
        "}\n",
        "\n",
        "response = requests.post(url, headers=headers, json=payload)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    result = response.json()\n",
        "    print('‚úì Embedding Generation Success!')\n",
        "    \n",
        "    if 'embedding' in result:\n",
        "        embedding = result['embedding'].get('values', [])\n",
        "        print(f\"\\nText: '{texts_to_embed[0]}'\")\n",
        "        print(f\"Embedding dimensions: {len(embedding)}\")\n",
        "        print(f\"First 10 values: {embedding[:10]}\")\n",
        "    else:\n",
        "        print(f\"Response: {result}\")\n",
        "else:\n",
        "    print(f'‚ö†Ô∏è  Embeddings may not be supported by gemini-2.0-flash')\n",
        "    print(f'Error {response.status_code}: {response.text}')\n",
        "    print('\\nNote: For embeddings, you may need to use a dedicated embedding model')\n",
        "    print('like text-embedding-004 or textembedding-gecko if available in your deployment.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Summary: All Gemini 2.0 Flash Endpoints\n",
        "\n",
        "| Endpoint | Purpose | URL Pattern |\n",
        "|----------|---------|-------------|\n",
        "| `:generateContent` | Synchronous text generation | `.../gemini-2.0-flash:generateContent` |\n",
        "| `:streamGenerateContent` | Real-time streaming responses | `.../gemini-2.0-flash:streamGenerateContent` |\n",
        "| `:countTokens` | Count tokens in prompts | `.../gemini-2.0-flash:countTokens` |\n",
        "| `:embedContent` | Generate embeddings (may not be supported) | `.../gemini-2.0-flash:embedContent` |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper Functions for All Endpoints\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GeminiClient:\n",
        "    \"\"\"Helper class for all Gemini 2.0 Flash endpoints\"\"\"\n",
        "    \n",
        "    def __init__(self, deployment_id='d83ad32cbda0399d', base_url=AI_API_URL, headers=None):\n",
        "        self.deployment_id = deployment_id\n",
        "        self.base_url = base_url\n",
        "        self.headers = headers or proxy.get_request_header()\n",
        "        self.model_name = 'gemini-2.0-flash'\n",
        "    \n",
        "    def generate(self, prompt, temperature=0.7, max_tokens=1024):\n",
        "        \"\"\"Standard synchronous generation\"\"\"\n",
        "        url = f'{self.base_url}/v2/inference/deployments/{self.deployment_id}/models/{self.model_name}:generateContent'\n",
        "        \n",
        "        payload = {\n",
        "            'contents': [{'role': 'user', 'parts': [{'text': prompt}]}],\n",
        "            'generationConfig': {\n",
        "                'temperature': temperature,\n",
        "                'maxOutputTokens': max_tokens\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        response = requests.post(url, headers=self.headers, json=payload)\n",
        "        \n",
        "        if response.status_code == 200:\n",
        "            return response.json()['candidates'][0]['content']['parts'][0]['text']\n",
        "        else:\n",
        "            raise Exception(f'Error {response.status_code}: {response.text}')\n",
        "    \n",
        "    def stream_generate(self, prompt, temperature=0.7, max_tokens=1024):\n",
        "        \"\"\"Streaming generation\"\"\"\n",
        "        url = f'{self.base_url}/v2/inference/deployments/{self.deployment_id}/models/{self.model_name}:streamGenerateContent'\n",
        "        \n",
        "        payload = {\n",
        "            'contents': [{'role': 'user', 'parts': [{'text': prompt}]}],\n",
        "            'generationConfig': {\n",
        "                'temperature': temperature,\n",
        "                'maxOutputTokens': max_tokens\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        response = requests.post(url, headers=self.headers, json=payload, stream=True)\n",
        "        \n",
        "        if response.status_code == 200:\n",
        "            import json\n",
        "            for line in response.iter_lines():\n",
        "                if line:\n",
        "                    try:\n",
        "                        chunk = json.loads(line.decode('utf-8'))\n",
        "                        if 'candidates' in chunk and len(chunk['candidates']) > 0:\n",
        "                            yield chunk['candidates'][0]['content']['parts'][0]['text']\n",
        "                    except json.JSONDecodeError:\n",
        "                        continue\n",
        "        else:\n",
        "            raise Exception(f'Error {response.status_code}: {response.text}')\n",
        "    \n",
        "    def count_tokens(self, prompt):\n",
        "        \"\"\"Count tokens in a prompt\"\"\"\n",
        "        url = f'{self.base_url}/v2/inference/deployments/{self.deployment_id}/models/{self.model_name}:countTokens'\n",
        "        \n",
        "        payload = {\n",
        "            'contents': [{'role': 'user', 'parts': [{'text': prompt}]}]\n",
        "        }\n",
        "        \n",
        "        response = requests.post(url, headers=self.headers, json=payload)\n",
        "        \n",
        "        if response.status_code == 200:\n",
        "            return response.json()\n",
        "        else:\n",
        "            raise Exception(f'Error {response.status_code}: {response.text}')\n",
        "    \n",
        "    def embed(self, text):\n",
        "        \"\"\"Generate embeddings (may not be supported)\"\"\"\n",
        "        url = f'{self.base_url}/v2/inference/deployments/{self.deployment_id}/models/{self.model_name}:embedContent'\n",
        "        \n",
        "        payload = {\n",
        "            'content': {'parts': [{'text': text}]}\n",
        "        }\n",
        "        \n",
        "        response = requests.post(url, headers=self.headers, json=payload)\n",
        "        \n",
        "        if response.status_code == 200:\n",
        "            return response.json()\n",
        "        else:\n",
        "            raise Exception(f'Error {response.status_code}: {response.text}')\n",
        "\n",
        "\n",
        "# Create client instance\n",
        "gemini = GeminiClient()\n",
        "\n",
        "print('‚úì GeminiClient initialized!')\n",
        "print('\\nUsage examples:')\n",
        "print('  result = gemini.generate(\"Your prompt here\")')\n",
        "print('  for chunk in gemini.stream_generate(\"Your prompt\"): print(chunk, end=\"\")')\n",
        "print('  tokens = gemini.count_tokens(\"Your prompt\")')\n",
        "print('  embedding = gemini.embed(\"Your text\")')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Demo: Using the GeminiClient Helper\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo 1: Standard generation\n",
        "print('=== Demo 1: Standard Generation ===')\n",
        "response = gemini.generate('Explain SAP AI Core in 2 sentences.', max_tokens=100)\n",
        "print(response)\n",
        "print()\n",
        "\n",
        "# Demo 2: Streaming generation\n",
        "print('=== Demo 2: Streaming Generation ===')\n",
        "print('Response: ', end='')\n",
        "for chunk in gemini.stream_generate('Count from 1 to 5 with descriptions.', max_tokens=150):\n",
        "    print(chunk, end='', flush=True)\n",
        "print('\\n')\n",
        "\n",
        "# Demo 3: Token counting\n",
        "print('=== Demo 3: Token Counting ===')\n",
        "test_text = \"How many tokens are in this sentence?\"\n",
        "token_info = gemini.count_tokens(test_text)\n",
        "print(f'Text: \"{test_text}\"')\n",
        "print(f'Token count: {token_info}')\n",
        "print()\n",
        "\n",
        "# Demo 4: Embeddings (may fail if not supported)\n",
        "print('=== Demo 4: Embeddings ===')\n",
        "try:\n",
        "    embedding_result = gemini.embed('SAP AI Core deployment')\n",
        "    print(f'‚úì Embedding generated: {embedding_result}')\n",
        "except Exception as e:\n",
        "    print(f'‚ö†Ô∏è  Embeddings not supported: {e}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Multi-turn Conversation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: What is SAP BTP?\n",
            "AI: Of course. Let's break down SAP BTP in a clear, structured way.\n",
            "\n",
            "### The Simple Analogy\n",
            "\n",
            "Imagine your core SAP system (like S/4HANA) is a brand-new, high-end smartphone. It's powerful and does its main job‚Äîrunning your business‚Äîexceptionally well.\n",
            "\n",
            "Now, you want to add new, custom features: a special app for your sales team, a unique dashboard for your CEO, or a way to connect to a new supplier's system.\n",
            "\n",
            "You have two choices:\n",
            "1.  **Jailbreak the Phone:** Hack the phone's core operating system. This might work, but it's risky, makes future updates a nightmare, and could break the phone.\n",
            "2.  **Use the App Store and Developer Tools:** Use the official tools and platform provided by the phone manufacturer to build and run new apps. These apps work seamlessly with the phone but run separately, keeping the core operating system clean, stable, and easy to update.\n",
            "\n",
            "**SAP BTP is the \"App Store and Developer Tools\" for your SAP landscape.** It's a platform that lets you build, integrate, and extend applications without dangerously modifying your core SAP system.\n",
            "\n",
            "---\n",
            "\n",
            "### The Formal Definition\n",
            "\n",
            "**SAP Business Technology Platform (BTP)** is SAP's **Platform-as-a-Service (PaaS)** offering. It is a cloud-based platform that bundles a wide array of services for data management, application development, integration, and intelligent technologies (like AI and Analytics) into a single, unified environment.\n",
            "\n",
            "Its primary goal is to help companies become an **\"Intelligent Enterprise\"** by enabling them to:\n",
            "*   **Integrate** SAP and non-SAP applications.\n",
            "*   **Extend** the functionality of their existing SAP solutions.\n",
            "*   **Innovate** by building entirely new applications and analyzing data in new ways.\n",
            "\n",
            "---\n",
            "\n",
            "### The Core Principle: \"Keep the Core Clean\"\n",
            "\n",
            "This is the most important concept to understand about BTP. In the past, customizing SAP ERP systems involved writing custom code directly within the core system. This made upgrades incredibly complex, expensive, and time-consuming because all that custom code had to be tested and rewritten.\n",
            "\n",
            "BTP solves this by providing a separate, cloud-based layer for all customizations and extensions. This means the core ERP system (like S/4HANA) remains standard or \"clean,\" allowing for faster, easier upgrades. All the custom logic, apps, and integrations live safely on BTP.\n",
            "\n",
            "---\n",
            "\n",
            "### The Five Pillars of SAP BTP\n",
            "\n",
            "SAP BTP is not a single product but a collection of services, often categorized into five main pillars:\n",
            "\n",
            "**1. Application Development and Automation**\n",
            "This is the \"build\" part of BTP. It provides tools and runtimes for creating new applications and automating processes.\n",
            "*   **Key Services:** SAP Build (low-code/no-code development), ABAP Cloud Environment (develop in the modern ABAP language, but in the cloud), Cloud Application Programming Model (CAP), SAP Build Process Automation (for workflow and RPA).\n",
            "\n",
            "**2. Data and Analytics**\n",
            "This pillar is focused on managing, governing, and deriving insights from data.\n",
            "*   **Key Services:** **SAP HANA Cloud** (powerful in-memory database-as-a-service), **SAP Analytics Cloud (SAC)** (for business intelligence, planning, and predictive analytics), **SAP Datasphere** (a comprehensive data service for data integration, cataloging, and warehousing).\n",
            "\n",
            "**3. Integration**\n",
            "This is the \"connect\" part of BTP. It ensures seamless data flow between different systems, whether they are cloud or on-premise, SAP or non-SAP.\n",
            "*   **Key Services:** **SAP Integration Suite**, which is a complete Integration Platform-as-a-Service (iPaaS). It includes pre-built connectors, API management, and tools to design data flows.\n",
            "\n",
            "**4. Artificial Intelligence (AI)**\n",
            "This pillar embeds intelligent technologies into business processes.\n",
            "*   **Key Services:** **SAP AI Business Services** (pre-trained models for tasks like document processing or service ticket classification), **SAP AI Core** (for developing and running your own custom AI/ML models).\n",
            "\n",
            "**5. BTP Foundation**\n",
            "While not always listed as a separate pillar, this is the technical foundation that underpins everything. It includes security, identity and access management, and the crucial **multi-cloud architecture**, which allows BTP to run on major hyperscalers like **Amazon Web Services (AWS), Microsoft Azure, Google Cloud Platform (GCP)**, and Alibaba Cloud.\n",
            "\n",
            "---\n",
            "\n",
            "### A Practical Example\n",
            "\n",
            "Imagine a company wants a mobile app for its sales managers to approve special discounts on the go.\n",
            "\n",
            "Here‚Äôs how they would use BTP:\n",
            "\n",
            "1.  **Integration:** They use the **SAP Integration Suite** on BTP to create a secure API that pulls sales order data from their core S/4HANA system.\n",
            "2.  **Application Development:** Using **SAP Build Apps** (a low-code tool) on BTP, a business analyst quickly designs and builds the mobile approval app. The app calls the API created in step 1.\n",
            "3.  **Data & Analytics:** The approval data is fed into **SAP Analytics Cloud** on BTP to create a real-time dashboard for the Head of Sales to track approval times and discount trends.\n",
            "4.  **Security:** BTP handles user authentication, ensuring only authorized managers can log in and approve discounts.\n",
            "\n",
            "**Result:** The company gets a powerful new mobile app without ever touching the core S/4HANA code. When it's time to upgrade S/4HANA, the process is smooth because the custom app is completely separate.\n",
            "\n",
            "### Summary\n",
            "\n",
            "| **In a Nutshell, SAP BTP is:** | |\n",
            "| :--- | :--- |\n",
            "| **What it is** | A Platform-as-a-Service (PaaS) in the cloud. |\n",
            "| **Its Main Purpose** | To **extend, integrate, and build** applications while **keeping the core clean**. |\n",
            "| **Key Components** | Services for **App Development, Data & Analytics, Integration, and AI**. |\n",
            "| **Where it Runs** | On major cloud providers like **AWS, Azure, and GCP**. |\n",
            "| **Why it Matters** | It is the technical foundation for modernizing and innovating around an SAP landscape, enabling agility and faster business transformation. |\n",
            "\n",
            "User: How does it relate to AI Core?\n",
            "AI: Excellent question. This gets to the heart of how SAP BTP delivers specialized capabilities.\n",
            "\n",
            "The relationship is simple and hierarchical:\n",
            "\n",
            "**SAP AI Core is a specific service that runs *on* the SAP Business Technology Platform (BTP).**\n",
            "\n",
            "Think of it like this:\n",
            "*   **SAP BTP** is the entire factory. It has the electricity, security, loading docks, assembly lines, and different workshops.\n",
            "*   **SAP AI Core** is the specialized, high-tech R&D workshop *inside* that factory, specifically equipped for building, training, and running custom Artificial Intelligence models.\n",
            "\n",
            "Let's break down that relationship in more detail.\n",
            "\n",
            "---\n",
            "\n",
            "### What is SAP AI Core?\n",
            "\n",
            "**SAP AI Core** is a service on BTP that provides the infrastructure and tools to manage the lifecycle of AI/Machine Learning models. It is **NOT** a tool for building models from scratch (like a data scientist's notebook). Instead, it is a robust, enterprise-grade system for:\n",
            "\n",
            "*   **Training:** Executing training scripts (written in Python using libraries like TensorFlow or PyTorch) on scalable infrastructure.\n",
            "*   **Deploying:** Taking a trained model and serving it as a live, callable API endpoint.\n",
            "*   **Inferencing:** Running new data through the deployed model to get predictions.\n",
            "*   **Monitoring & Management:** Tracking model versions, performance, and operational health.\n",
            "\n",
            "Essentially, AI Core is the bridge between a data scientist's code and a production-ready, scalable AI application. It handles the complex \"AI-ops\" (MLOps) part of the process, which is often the hardest part of implementing AI.\n",
            "\n",
            "---\n",
            "\n",
            "### How SAP BTP and SAP AI Core Work Together\n",
            "\n",
            "SAP BTP provides the essential foundation and surrounding services that make AI Core useful in a business context. Here‚Äôs how they relate across different functions:\n",
            "\n",
            "**1. Foundation and Infrastructure**\n",
            "*   **BTP Provides the Runtime:** AI Core doesn't run in a vacuum. It runs on BTP's underlying infrastructure, which is built on Kubernetes. BTP manages all this complexity, so the developer using AI Core doesn't need to be a Kubernetes expert. They just interact with the AI Core APIs.\n",
            "*   **BTP Provides Security:** BTP handles user authentication, authorization, and secure connectivity, ensuring that only the right people and applications can access your AI models.\n",
            "\n",
            "**2. Data and Integration**\n",
            "*   **AI Models Need Data:** An AI model is useless without data for training and inferencing. This is where BTP's other pillars shine.\n",
            "*   **Integration Suite:** You use the **SAP Integration Suite** (on BTP) to create data pipelines that pull training data from your S/4HANA, SuccessFactors, or any other system into a place where AI Core can access it.\n",
            "*   **SAP Datasphere:** You might use **SAP Datasphere** (on BTP) to cleanse, prepare, and stage this data before feeding it to your AI model in AI Core.\n",
            "\n",
            "**3. Application Development**\n",
            "*   **AI Models Need a User Interface:** An AI model just provides a prediction (e.g., a number, a category). It's not a complete business application.\n",
            "*   **SAP Build / CAP:** You use BTP's application development tools (like SAP Build Apps for low-code or the Cloud Application Programming Model) to build the user-facing application. This application is what calls the AI model deployed on AI Core and presents the results to a business user in a meaningful way.\n",
            "\n",
            "### A Practical Example: Predictive Maintenance\n",
            "\n",
            "Imagine you want to build an AI model that predicts when a piece of machinery will fail.\n",
            "\n",
            "1.  **Get the Data (BTP Integration):** You use the **SAP Integration Suite** on BTP to pull years of sensor data (temperature, vibration) and maintenance records from your SAP S/4HANA Plant Maintenance module.\n",
            "2.  **Prepare the Data (BTP Data & Analytics):** You use **SAP Datasphere** on BTP to clean and structure this data into a format suitable for training an AI model.\n",
            "3.  **Train & Deploy the Model (AI Core on BTP):** A data scientist writes a Python script for a prediction model. They use **SAP AI Core** to execute this script, training the model on the historical data. Once trained, they use AI Core to deploy this model as a live API.\n",
            "4.  **Build the App (BTP Application Development):** A developer uses **SAP Build Apps** on BTP to create a simple dashboard for the plant manager.\n",
            "5.  **Run the Process (Putting it all together):**\n",
            "    *   Every hour, a job scheduled on BTP pulls live sensor data from the machine.\n",
            "    *   It sends this data to the model's API endpoint on **AI Core**.\n",
            "    *   AI Core returns a prediction: \"85% probability of failure in the next 48 hours.\"\n",
            "    *   The dashboard built on BTP sees this high probability, flags the machine in red, and automatically creates a high-priority maintenance notification in S/4HANA via the **Integration Suite**.\n",
            "\n",
            "---\n",
            "\n",
            "### Important Distinction: AI Core vs. AI Business Services\n",
            "\n",
            "BTP offers two main ways to use AI, which often causes confusion:\n",
            "\n",
            "| | **SAP AI Core** | **SAP AI Business Services** |\n",
            "| :--- | :--- | :--- |\n",
            "| **Purpose** | A \"workshop\" to run your own **custom-built** AI models. | A set of **pre-built, ready-to-use** AI models for common business tasks. |\n",
            "| **Analogy** | The professional engine-building toolkit. | The off-the-shelf, high-performance engine you can just plug in. |\n",
            "| **Use Case** | Predicting customer-specific churn; finding unique fraud patterns in your data. | Extracting data from standard invoices; classifying service tickets; recognizing entities in text. |\n",
            "| **Who Uses It** | Data Scientists and MLOps Engineers. | Application Developers. |\n",
            "| **Relationship to BTP** | **Both are services that run on SAP BTP.** | **Both are services that run on SAP BTP.** |\n",
            "\n",
            "In summary, **SAP AI Core is a powerful, specialized service *within* the broader SAP BTP ecosystem**, designed for companies that need to build and operate their own unique AI models and embed them deeply into their business processes.\n"
          ]
        }
      ],
      "source": [
        "# Conversation with history\n",
        "DEPLOYMENT_ID = 'd6ae523ed14c6cc3'\n",
        "url = f'{base_url}/v2/inference/deployments/{DEPLOYMENT_ID}/models/gemini-2.5-pro:generateContent'\n",
        "\n",
        "# Build conversation\n",
        "contents = [\n",
        "    {'role': 'user', 'parts': [{'text': 'What is SAP BTP?'}]}\n",
        "]\n",
        "\n",
        "resp = requests.post(url, headers=headers, json={'contents': contents})\n",
        "reply1 = resp.json()['candidates'][0]['content']['parts'][0]['text']\n",
        "print('User: What is SAP BTP?')\n",
        "print(f'AI: {reply1}\\n')\n",
        "\n",
        "# Continue conversation\n",
        "contents.append({'role': 'model', 'parts': [{'text': reply1}]})\n",
        "contents.append({'role': 'user', 'parts': [{'text': 'How does it relate to AI Core?'}]})\n",
        "\n",
        "resp = requests.post(url, headers=headers, json={'contents': contents})\n",
        "reply2 = resp.json()['candidates'][0]['content']['parts'][0]['text']\n",
        "print('User: How does it relate to AI Core?')\n",
        "print(f'AI: {reply2}')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
